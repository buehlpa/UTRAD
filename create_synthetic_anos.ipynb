{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/HuiZhang0812/DiffusionAD/blob/main/data/dataset_beta_thresh.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.dataloader import *\n",
    "from models import Create_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import cv2\n",
    "import glob\n",
    "import imgaug.augmenters as iaa\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# from data.perlin import rand_perlin_2d_np\n",
    "\n",
    "texture_list = ['carpet', 'zipper', 'leather', 'tile', 'wood','grid',\n",
    "                'Class1', 'Class2', 'Class3', 'Class4', 'Class5',\n",
    "                'Class6', 'Class7', 'Class8', 'Class9', 'Class10']\n",
    "\n",
    "category='carpet'\n",
    "\n",
    "anomaly_categories={\n",
    "    \"bottle\": [\"broken_large\", \"broken_small\", \"contamination\"],\n",
    "    \"cable\": [\"bent_wire\", \"cable_swap\", \"combined\", \"cut_inner_insulation\", \"cut_outer_insulation\", \"missing_cable\", \"missing_wire\", \"poke_insulation\"],\n",
    "    \"capsule\": [\"crack\", \"faulty_imprint\", \"poke\", \"scratch\",\"squeeze\"],\n",
    "    \"carpet\": [\"color\", \"cut\", \"hole\", \"metal_contamination\", \"thread\"],\n",
    "    \"grid\": [\"bent\", \"broken\", \"glue\", \"metal_contamination\", \"thread\"],\n",
    "    \"hazelnut\": [\"crack\", \"cut\", \"hole\", \"print\"],\n",
    "    \"leather\": [\"color\", \"cut\", \"fold\", \"glue\", \"poke\"],\n",
    "    \"metal_nut\": [\"bent\", \"color\", \"flip\", \"scratch\"],\n",
    "    \"pill\": [\"color\", \"combined\",\"contamination\", \"crack\", \"faulty_imprint\", \"pill_type\",\"scratch\"],\n",
    "    \"screw\": [\"manipulated_front\", \"scratch_head\", \"scratch_neck\",\"thread_side\", \"thread_top\"],\n",
    "    \"tile\": [\"crack\", \"glue_strip\", \"gray_stroke\", \"oil\",\"rough\"],\n",
    "    \"toothbrush\": [\"defective\"],\n",
    "    \"transistor\": [\"bent_lead\", \"cut_lead\", \"damaged_case\", \"misplaced\"],\n",
    "    \"wood\": [\"color\", \"combined\", \"hole\", \"liquid\", \"scratch\"],\n",
    "    \"zipper\": [\"broken_teeth\", \"combined\",\"fabric_border\", \"fabric_interior\",\"split_teeth\",\"rough\", \"squeezed_teeth\"]}\n",
    "\n",
    "\n",
    "#args = TrainOptions().parse() # surpass kernelerror with this:\n",
    "class TrainOptions:\n",
    "    def __init__(self, category='carpet'):\n",
    "        self.exp_name = \"DEV_DATALOADER\"\n",
    "        self.epoch_start = 0\n",
    "        self.epoch_num = 150\n",
    "        self.factor = 1\n",
    "        self.seed = 233\n",
    "        self.fixed_seed_bool = False\n",
    "        self.test_seed = 400\n",
    "        self.data_ratio = 0.5\n",
    "        self.num_row = 4\n",
    "        self.activation = 'gelu'\n",
    "        self.unalign_test = False\n",
    "        self.data_root = '/home/bule/projects/datasets/mvtec_anomaly_detection/'\n",
    "        self.synthetic_anomaly_root = '/home/bule/projects/datasets/dtd'\n",
    "        self.data_category = category\n",
    "        self.batch_size = 2\n",
    "        self.lr = 1e-4\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 8\n",
    "        self.image_result_dir = 'result_images'\n",
    "        self.model_result_dir = 'saved_models'\n",
    "        self.validation_image_dir = 'validation_images'\n",
    "        self.contamination_rate = 0.0\n",
    "        self.validation= 0.0\n",
    "        self.data_set = 'mvtec'\n",
    "        self.mode = 'mvtec'\n",
    "        self.results_dir = 'results'\n",
    "        self.development = False\n",
    "        self.use_synthetic=True\n",
    "        self.synthetic_ratio=0.5\n",
    "\n",
    "args = TrainOptions()\n",
    "torch.manual_seed(args.seed)\n",
    "        \n",
    "DATA_PATH=os.path.join(args.data_root,args.data_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef rand_perlin_2d(shape, res, fade=lambda t: 6 * t ** 5 - 15 * t ** 4 + 10 * t ** 3):\\n    delta = (res[0] / shape[0], res[1] / shape[1])\\n    d = (shape[0] // res[0], shape[1] // res[1])\\n\\n    #grid = torch.stack(torch.meshgrid(torch.arange(0, res[0], delta[0]), torch.arange(0, res[1], delta[1]), indexing='ij'), dim=-1) % 1\\n    grid = torch.stack(torch.meshgrid(torch.arange(0, res[0], delta[0]), torch.arange(\\n        0, res[1], delta[1])), dim=-1) % 1    \\n    angles = 2 * math.pi * torch.rand(res[0] + 1, res[1] + 1)\\n    gradients = torch.stack((torch.cos(angles), torch.sin(angles)), dim=-1)\\n\\n    tile_grads = lambda slice1, slice2: gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]].repeat_interleave(d[0],\\n                                                                                                              0).repeat_interleave(\\n        d[1], 1)\\n    dot = lambda grad, shift: (\\n                torch.stack((grid[:shape[0], :shape[1], 0] + shift[0], grid[:shape[0], :shape[1], 1] + shift[1]),\\n                            dim=-1) * grad[:shape[0], :shape[1]]).sum(dim=-1)\\n\\n    n00 = dot(tile_grads([0, -1], [0, -1]), [0, 0])\\n\\n    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])\\n    n01 = dot(tile_grads([0, -1], [1, None]), [0, -1])\\n    n11 = dot(tile_grads([1, None], [1, None]), [-1, -1])\\n    t = fade(grid[:shape[0], :shape[1]])\\n    return math.sqrt(2) * torch.lerp(torch.lerp(n00, n10, t[..., 0]), torch.lerp(n01, n11, t[..., 0]), t[..., 1])\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def lerp_np(x,y,w):\n",
    "    fin_out = (y-x)*w + x\n",
    "    return fin_out\n",
    "\n",
    "def generate_fractal_noise_2d(shape, res, octaves=1, persistence=0.5):\n",
    "    noise = np.zeros(shape)\n",
    "    frequency = 1\n",
    "    amplitude = 1\n",
    "    for _ in range(octaves):\n",
    "        noise += amplitude * generate_perlin_noise_2d(shape, (frequency*res[0], frequency*res[1]))\n",
    "        frequency *= 2\n",
    "        amplitude *= persistence\n",
    "    return noise\n",
    "\n",
    "\n",
    "def generate_perlin_noise_2d(shape, res):\n",
    "    def f(t):\n",
    "        return 6 * t ** 5 - 15 * t ** 4 + 10 * t ** 3\n",
    "\n",
    "    delta = (res[0] / shape[0], res[1] / shape[1])\n",
    "    d = (shape[0] // res[0], shape[1] // res[1])\n",
    "    grid = np.mgrid[0:res[0]:delta[0], 0:res[1]:delta[1]].transpose(1, 2, 0) % 1\n",
    "    # Gradients\n",
    "    angles = 2 * np.pi * np.random.rand(res[0] + 1, res[1] + 1)\n",
    "    gradients = np.dstack((np.cos(angles), np.sin(angles)))\n",
    "    g00 = gradients[0:-1, 0:-1].repeat(d[0], 0).repeat(d[1], 1)\n",
    "    g10 = gradients[1:, 0:-1].repeat(d[0], 0).repeat(d[1], 1)\n",
    "    g01 = gradients[0:-1, 1:].repeat(d[0], 0).repeat(d[1], 1)\n",
    "    g11 = gradients[1:, 1:].repeat(d[0], 0).repeat(d[1], 1)\n",
    "    # Ramps\n",
    "    n00 = np.sum(grid * g00, 2)\n",
    "    n10 = np.sum(np.dstack((grid[:, :, 0] - 1, grid[:, :, 1])) * g10, 2)\n",
    "    n01 = np.sum(np.dstack((grid[:, :, 0], grid[:, :, 1] - 1)) * g01, 2)\n",
    "    n11 = np.sum(np.dstack((grid[:, :, 0] - 1, grid[:, :, 1] - 1)) * g11, 2)\n",
    "    # Interpolation\n",
    "    t = f(grid)\n",
    "    n0 = n00 * (1 - t[:, :, 0]) + t[:, :, 0] * n10\n",
    "    n1 = n01 * (1 - t[:, :, 0]) + t[:, :, 0] * n11\n",
    "    return np.sqrt(2) * ((1 - t[:, :, 1]) * n0 + t[:, :, 1] * n1)\n",
    "\n",
    "\n",
    "def rand_perlin_2d_np(shape, res, fade=lambda t: 6 * t ** 5 - 15 * t ** 4 + 10 * t ** 3):  #shape (256 256) res (16,2))\n",
    "    delta = (res[0] / shape[0], res[1] / shape[1]) #(1/16,1,128)\n",
    "    d = (shape[0] // res[0], shape[1] // res[1])  #(16,128)\n",
    "    grid = np.mgrid[0:res[0]:delta[0], 0:res[1]:delta[1]].transpose(1, 2, 0) % 1   #delta 为间隔 0:res[0]为上下界。 (256,256,2)\n",
    "\n",
    "    angles = 2 * math.pi * np.random.rand(res[0] + 1, res[1] + 1)    #(17,3)\n",
    "    gradients = np.stack((np.cos(angles), np.sin(angles)), axis=-1)  #(17,3,2)\n",
    "    tt = np.repeat(np.repeat(gradients,d[0],axis=0),d[1],axis=1) # (272,384,2)\n",
    "\n",
    "    tile_grads = lambda slice1, slice2: np.repeat(np.repeat(gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]],d[0],axis=0),d[1],axis=1)\n",
    "    dot = lambda grad, shift: (\n",
    "                np.stack((grid[:shape[0], :shape[1], 0] + shift[0], grid[:shape[0], :shape[1], 1] + shift[1]),\n",
    "                            axis=-1) * grad[:shape[0], :shape[1]]).sum(axis=-1)\n",
    "\n",
    "    n00 = dot(tile_grads([0, -1], [0, -1]), [0, 0]) #(256,256)\n",
    "    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])\n",
    "    n01 = dot(tile_grads([0, -1], [1, None]), [0, -1])\n",
    "    n11 = dot(tile_grads([1, None], [1, None]), [-1, -1])\n",
    "    t = fade(grid[:shape[0], :shape[1]]) #(256,256,2)\n",
    "    return math.sqrt(2) * lerp_np(lerp_np(n00, n10, t[..., 0]), lerp_np(n01, n11, t[..., 0]), t[..., 1]) #(256,256)\n",
    "\n",
    "\"\"\"\n",
    "def rand_perlin_2d(shape, res, fade=lambda t: 6 * t ** 5 - 15 * t ** 4 + 10 * t ** 3):\n",
    "    delta = (res[0] / shape[0], res[1] / shape[1])\n",
    "    d = (shape[0] // res[0], shape[1] // res[1])\n",
    "\n",
    "    #grid = torch.stack(torch.meshgrid(torch.arange(0, res[0], delta[0]), torch.arange(0, res[1], delta[1]), indexing='ij'), dim=-1) % 1\n",
    "    grid = torch.stack(torch.meshgrid(torch.arange(0, res[0], delta[0]), torch.arange(\n",
    "        0, res[1], delta[1])), dim=-1) % 1    \n",
    "    angles = 2 * math.pi * torch.rand(res[0] + 1, res[1] + 1)\n",
    "    gradients = torch.stack((torch.cos(angles), torch.sin(angles)), dim=-1)\n",
    "\n",
    "    tile_grads = lambda slice1, slice2: gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]].repeat_interleave(d[0],\n",
    "                                                                                                              0).repeat_interleave(\n",
    "        d[1], 1)\n",
    "    dot = lambda grad, shift: (\n",
    "                torch.stack((grid[:shape[0], :shape[1], 0] + shift[0], grid[:shape[0], :shape[1], 1] + shift[1]),\n",
    "                            dim=-1) * grad[:shape[0], :shape[1]]).sum(dim=-1)\n",
    "\n",
    "    n00 = dot(tile_grads([0, -1], [0, -1]), [0, 0])\n",
    "\n",
    "    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])\n",
    "    n01 = dot(tile_grads([0, -1], [1, None]), [0, -1])\n",
    "    n11 = dot(tile_grads([1, None], [1, None]), [-1, -1])\n",
    "    t = fade(grid[:shape[0], :shape[1]])\n",
    "    return math.sqrt(2) * torch.lerp(torch.lerp(n00, n10, t[..., 0]), torch.lerp(n01, n11, t[..., 0]), t[..., 1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: carpet, normals train:  298, anomalies test: 89, normal test: 28\n",
      "anomalies test total:     {'color': 19, 'cut': 17, 'hole': 17, 'metal_contamination': 17, 'thread': 19}\n",
      "anomalies test sampled:   {'color': 0, 'cut': 0, 'hole': 0, 'metal_contamination': 0, 'thread': 0}\n",
      "anomalies test remaining: {'color': 19, 'cut': 17, 'hole': 17, 'metal_contamination': 17, 'thread': 19}\n",
      "['/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/236.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/182.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/087.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/127.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/148.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/054.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/112.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/237.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/205.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/100.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/081.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/270.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/230.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/036.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/235.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/068.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/184.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/223.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/118.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/001.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/077.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/108.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/173.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/171.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/224.png_aasepkua.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/269.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/110.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/131.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/049.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/226.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/247.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/229.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/076.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/093.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/035.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/032.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/115.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/178.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/133.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/241.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/240.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/109.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/141.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/102.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/074.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/051.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/262.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/246.png_mvgm4js8.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/039.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/129.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/019.png_rsmbbe2x.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/084.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/224.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/172.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/175.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/011.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/190.png_n5ld0z1g.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/069.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/200.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/168.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/012.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/179.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/206.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/124.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/218.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/059.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/061.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/244.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/071.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/104.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/276.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/021.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/222.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/167.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/261.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/220.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/096.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/157.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/082.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/142.png_1od9f22z.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/194.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/013.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/176.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/158.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/085.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/045.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/062.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/019.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/273.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/159.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/275.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/094.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/098.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/058.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/119.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/221.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/075.png_2862l8yn.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/009.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/107.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/228.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/257.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/090.png_tytmohm1.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/065.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/041.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/149.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/028.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/239.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/007.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/245.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/191.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/263.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/163.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/130.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/183.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/106.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/165.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/181.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/231.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/055.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/277.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/174.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/003.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/201.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/203.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/023.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/122.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/160.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/071.png_klleeo18.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/143.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/234.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/016.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/180.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/251.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/155.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/250.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/150.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/248.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/254.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/253.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/243.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/159.png_3eqaxm05.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/188.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/080.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/217.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/219.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/212.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/272.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/162.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/144.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/154.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/274.png_2owdqdl7.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/048.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/213.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/123.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/033.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/137.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/214.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/260.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/101.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/233.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/207.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/025.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/126.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/014.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/199.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/095.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/075.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/256.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/227.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/039.png_aw114cu2.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/187.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/034.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/010.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/145.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/146.png_ni7tejp4.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/249.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/136.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/196.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/151.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/000.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/202.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/037.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/140.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/209.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/116.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/198.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/204.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/152.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/047.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/060.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/063.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/120.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/265.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/044.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/252.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/197.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/153.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/211.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/142.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/132.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/138.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/274.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/056.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/271.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/225.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/004.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/192.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/026.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/022.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/027.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/242.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/170.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/042.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/083.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/208.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/053.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/232.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/161.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/031.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/267.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/241.png_597646z5.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/024.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/089.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/268.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/125.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/114.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/266.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/043.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/255.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/105.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/078.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/050.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/166.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/052.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/117.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/147.png_y4aurzzu.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/057.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/189.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/006.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/111.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/264.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/193.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/029.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/169.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/147.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/134.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/121.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/156.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/005.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/046.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/164.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/103.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/090.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/008.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/038.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/002.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/073.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/030.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/097.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/040.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/161.png_re6t2mqc.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/072.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/064.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/186.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/091.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/092.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/258.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/185.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/088.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/165.png_c8a96h6s.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/079.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/066.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/215.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/238.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/128.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/112.png_wn1urd63.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/086.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/279.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/278.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/139.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/259.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/113.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/099.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/246.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/146.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/177.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/067.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/252.png_8irmj3yv.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/190.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/020.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/216.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/018.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/015.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/135.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/017.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/195.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/210.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/070.png']\n",
      "Number of images in train mode: 298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/012.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/015.png\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/032.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/031.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/003.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/020.png\n",
      "\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32texture selected\n",
      "\n",
      "(3, 256, 256)(3, 256, 256)\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32texture selected\n",
      "texture selected\n",
      "\n",
      "texture selected\n",
      "(3, 256, 256)(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/025.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/007.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/031.png\n",
      "\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/023.png\n",
      "texture selected\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/015.png(3, 256, 256)thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/017.png\n",
      "\n",
      "\n",
      "texture selectedtype of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "texture selected\n",
      "texture selected(3, 256, 256)\n",
      "\n",
      "texture selectedtype of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "(3, 256, 256)\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/005.png\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/022.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/026.png\n",
      "(3, 256, 256)texture selected\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/014.png\n",
      "\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/022.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/024.png\n",
      "\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "texture selected\n",
      "texture selected\n",
      "(3, 256, 256)(3, 256, 256)\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "(3, 256, 256)texture selected\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "(3, 256, 256)thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/018.png\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "texture selected\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/030.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/019.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/028.png\n",
      "\n",
      "(3, 256, 256)\n",
      "texture selected\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/001.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/032.png\n",
      "texture selectedtexture selectedtexture selected\n",
      "\n",
      "\n",
      "(3, 256, 256)(3, 256, 256)\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/022.pngtexture selected\n",
      "\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/004.png\n",
      "texture selectedthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/025.png\n",
      "\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "texture_list = ['carpet', 'zipper', 'leather', 'tile', 'wood','grid',\n",
    "                'Class1', 'Class2', 'Class3', 'Class4', 'Class5',\n",
    "                 'Class6', 'Class7', 'Class8', 'Class9', 'Class10']\n",
    "\n",
    "class MVTecSynthAnoDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args, root, transforms_=None, mode='train', train_paths=None, test_paths=None):\n",
    "\n",
    "\n",
    "        #data_path,classname,img_size,args\n",
    "\n",
    "        ##################### origianl loader\n",
    "        self.img_size = 280 * args.factor\n",
    "        self.crop_size = 256 * args.factor\n",
    "        self.args = args\n",
    "        self.mode = mode\n",
    "        if train_paths is None and test_paths is None:\n",
    "            raise ValueError(\"either test or train paths must be provided depending on the mode\")\n",
    "        \n",
    "        self.train_paths = train_paths\n",
    "        self.test_paths = test_paths\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.files = train_paths\n",
    "            \n",
    "        elif mode == 'test':\n",
    "            self.files = test_paths\n",
    "            \n",
    "        \n",
    "        print(f\"Number of images in {mode} mode: {len(self.files)}\")\n",
    "        \n",
    "        # self.transform_train = transforms.Compose([ transforms.Resize((self.crop_size, self.crop_size), Image.BICUBIC),\n",
    "        #                                         transforms.Pad(int(self.crop_size/10),fill=0,padding_mode='constant'),\n",
    "        #                                         transforms.RandomRotation(10),\n",
    "        #                                         transforms.RandomCrop((self.crop_size, self.crop_size)),\n",
    "        #                                         transforms.ToTensor(),\n",
    "        #                                         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                                             std=[0.229, 0.224, 0.225 ]) ])\n",
    "        self.transform_train = transforms.Compose([ transforms.Resize((self.crop_size, self.crop_size), Image.BICUBIC),\n",
    "                                        # transforms.Pad(int(self.crop_size/10),fill=0,padding_mode='constant'),\n",
    "                                        # transforms.RandomRotation(10),\n",
    "                                        # transforms.RandomCrop((self.crop_size, self.crop_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225 ]) ])\n",
    "        \n",
    "        \n",
    "        ########################################\n",
    "\n",
    "        self.resize_transform_loco = transforms.Resize((self.crop_size, self.crop_size), Image.BICUBIC)\n",
    "\n",
    "        self.classname=args.data_category\n",
    "        \n",
    "        self.root_dir = os.path.join(root,'train','good')\n",
    "        self.resize_shape = [self.crop_size, self.crop_size]\n",
    "        self.anomaly_source_path = args.synthetic_anomaly_root\n",
    "        \n",
    "        \n",
    "\n",
    "        self.image_paths = train_paths\n",
    "        self.anomaly_source_paths = sorted(glob.glob(self.anomaly_source_path+\"/images/*/*.jpg\"))\n",
    "\n",
    "        self.augmenters = [iaa.GammaContrast((0.5, 2.0), per_channel=True),\n",
    "                           iaa.MultiplyAndAddToBrightness(\n",
    "                               mul=(0.8, 1.2), add=(-30, 30)),\n",
    "                           iaa.pillike.EnhanceSharpness(),\n",
    "                           iaa.AddToHueAndSaturation(\n",
    "                               (-50, 50), per_channel=True),\n",
    "                           iaa.Solarize(0.5, threshold=(32, 128)),\n",
    "                           iaa.Posterize(),\n",
    "                           iaa.Invert(),\n",
    "                           iaa.pillike.Autocontrast(),\n",
    "                           iaa.pillike.Equalize(),\n",
    "                           iaa.Affine(rotate=(-45, 45))\n",
    "                           ]\n",
    "        \n",
    "        self.augmenters_anomaly = [iaa.GammaContrast((0.5, 2.0), per_channel=True),\n",
    "                           iaa.MultiplyAndAddToBrightness(\n",
    "                               mul=(0.8, 1.2), add=(-30, 30)),\n",
    "                           iaa.pillike.EnhanceSharpness(),\n",
    "                           iaa.AddToHueAndSaturation(\n",
    "                               (-50, 50), per_channel=True),\n",
    "                           iaa.Solarize(0.5, threshold=(32, 128)),\n",
    "                           iaa.Posterize(),\n",
    "                           iaa.Invert(),\n",
    "                           iaa.pillike.Autocontrast(),\n",
    "                           iaa.pillike.Equalize(),\n",
    "                           ]\n",
    "\n",
    "        self.augmenters_mask = [iaa.Affine(rotate=(-90, 90)),\n",
    "                              iaa.Affine(shear=(0, 40)),\n",
    "                           iaa.Affine(translate_percent={\"x\": (-0.5, 0.5), \"y\": (-0.5, 0.5)}),]\n",
    "        \n",
    "        self.rot = iaa.Sequential([iaa.Affine(rotate=(-90, 90))])\n",
    "        \n",
    "\n",
    "        #foreground path of textural classes\n",
    "        foreground_path = os.path.join(args.data_root,'carpet')\n",
    "        self.textural_foreground_path = sorted(glob.glob(foreground_path +\"/thresh/*.png\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def _align_transform(self, img, mask):\n",
    "        #resize to 224\n",
    "        img = TF.resize(img, self.crop_size, Image.BICUBIC)\n",
    "        mask = TF.resize(mask, self.crop_size, Image.NEAREST)\n",
    "        #toTensor\n",
    "        img = TF.to_tensor(img)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        #normalize\n",
    "        img = TF.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225 ])\n",
    "        return img, mask\n",
    "\n",
    "    def _unalign_transform(self, img, mask):\n",
    "        #resize to 256\n",
    "        img = TF.resize(img, self.img_size, Image.BICUBIC)\n",
    "        mask = TF.resize(mask, self.img_size, Image.NEAREST)\n",
    "        #random rotation\n",
    "        angle = transforms.RandomRotation.get_params([-10, 10])\n",
    "        img = TF.rotate(img, angle, fill=(0,))\n",
    "        mask = TF.rotate(mask, angle, fill=(0,))\n",
    "        #random crop\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(img, output_size=(self.crop_size, self.crop_size))\n",
    "        img = TF.crop(img, i, j, h, w)\n",
    "        mask = TF.crop(mask, i, j, h, w)\n",
    "        #toTensor\n",
    "        img = TF.to_tensor(img)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        #normalize\n",
    "        img = TF.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225 ])\n",
    "        return img, mask\n",
    "\n",
    "    def random_choice_foreground_path(self):\n",
    "        foreground_path_id = torch.randint(0, len(self.textural_foreground_path), (1,)).item()\n",
    "        foreground_path = self.textural_foreground_path[foreground_path_id]\n",
    "        return foreground_path\n",
    "\n",
    "\n",
    "    def get_foreground_mvtec(self,image_path):\n",
    "        classname = self.classname\n",
    "        if classname in texture_list:\n",
    "            foreground_path = self.random_choice_foreground_path()\n",
    "        else:\n",
    "            foreground_path = image_path.replace('train', 'DISthresh')\n",
    "        return foreground_path\n",
    "\n",
    "\n",
    "\n",
    "    def randAugmenter_anomaly(self):\n",
    "        aug_ind = np.random.choice(\n",
    "            np.arange(len(self.augmenters_anomaly)), 2, replace=False)\n",
    "        aug = iaa.Sequential([self.augmenters_anomaly[aug_ind[0]],\n",
    "                              self.augmenters_anomaly[aug_ind[1]]]\n",
    "                             )\n",
    "        return aug\n",
    "\n",
    "    def randAugmenter_mask(self):\n",
    "        aug_ind = np.random.choice(\n",
    "            np.arange(len(self.augmenters_mask)), 1, replace=False)\n",
    "        aug = iaa.Sequential([self.augmenters_mask[aug_ind[0]],]\n",
    "                             )\n",
    "        return aug\n",
    "\n",
    "\n",
    "    def randAugmenter(self):\n",
    "        aug_ind = np.random.choice(\n",
    "            np.arange(len(self.augmenters)), 3, replace=False)\n",
    "        aug = iaa.Sequential([self.augmenters[aug_ind[0]],\n",
    "                              self.augmenters[aug_ind[1]],\n",
    "                              self.augmenters[aug_ind[2]]]\n",
    "                             )\n",
    "        return aug\n",
    "\n",
    "\n",
    "    def perlin_synthetic(self, image, thresh, anomaly_source_path, cv2_image,thresh_path):\n",
    "\n",
    "    # no_anomaly = torch.rand(1).numpy()[0]\n",
    "    # if no_anomaly > 0.5:\n",
    "    #     image = image.astype(np.float32)\n",
    "    #     return image, np.zeros((self.resize_shape[0], self.resize_shape[1], 1), dtype=np.float32), np.array([0.0], dtype=np.float32)\n",
    "\n",
    "    # else:\n",
    "        perlin_scale = 6  \n",
    "        min_perlin_scale = 0\n",
    "        perlin_scalex = 2 ** (torch.randint(min_perlin_scale,perlin_scale, (1,)).numpy()[0])\n",
    "        perlin_scaley = 2 ** (torch.randint(min_perlin_scale,perlin_scale, (1,)).numpy()[0])\n",
    "\n",
    "        has_anomaly = 0\n",
    "        try_cnt = 0\n",
    "        while(has_anomaly == 0 and try_cnt<50):  \n",
    "            perlin_noise = rand_perlin_2d_np(\n",
    "                (self.resize_shape[0], self.resize_shape[1]), (perlin_scalex, perlin_scaley))\n",
    "            perlin_noise = self.rot(image=perlin_noise)\n",
    "            threshold = 0.5\n",
    "            perlin_thr = np.where(perlin_noise > threshold, np.ones_like(perlin_noise), np.zeros_like(perlin_noise))\n",
    "            \n",
    "            object_perlin = thresh*perlin_thr\n",
    "\n",
    "            object_perlin = np.expand_dims(object_perlin, axis=2).astype(np.float32)  \n",
    "\n",
    "            msk = (object_perlin).astype(np.float32) \n",
    "            if np.sum(msk) !=0: \n",
    "                has_anomaly = 1        \n",
    "            try_cnt+=1\n",
    "            \n",
    "        \n",
    "        if self.classname in texture_list: # only DTD\n",
    "            print('texture selected')\n",
    "            aug = self.randAugmenter()\n",
    "            anomaly_source_img = cv2.cvtColor(cv2.imread(anomaly_source_path),cv2.COLOR_BGR2RGB)\n",
    "            anomaly_source_img = cv2.resize(anomaly_source_img, dsize=(\n",
    "                self.resize_shape[1], self.resize_shape[0]))\n",
    "            anomaly_img_augmented = anomaly_source_img# aug(image=anomaly_source_img)# no aug\n",
    "            img_object_thr = anomaly_img_augmented.astype(\n",
    "                np.float32) * object_perlin/255.0\n",
    "            \n",
    "        else: # DTD and self-augmentation\n",
    "            texture_or_patch = torch.rand(1).numpy()[0]\n",
    "            if texture_or_patch > 0.5:  # >0.5 is DTD \n",
    "                aug = self.randAugmenter()\n",
    "                anomaly_source_img = cv2.cvtColor(cv2.imread(anomaly_source_path),cv2.COLOR_BGR2RGB)\n",
    "                anomaly_source_img = cv2.resize(anomaly_source_img, dsize=(\n",
    "                    self.resize_shape[1], self.resize_shape[0]))\n",
    "                anomaly_img_augmented = anomaly_source_img#aug(image=anomaly_source_img)\n",
    "                anomaly_img_augmented = anomaly_source_img# aug(image=anomaly_source_img)# no aug\n",
    "                img_object_thr = anomaly_img_augmented.astype(\n",
    "                    np.float32) * object_perlin/255.0\n",
    "\n",
    "            else: #self-augmentation\n",
    "                aug = self.randAugmenter()\n",
    "                anomaly_image = cv2_image#aug(image=cv2_image)#no aug\n",
    "                high, width = anomaly_image.shape[0], anomaly_image.shape[1]\n",
    "                gird_high, gird_width = int(high/8), int(width/8)\n",
    "                wi = np.split(anomaly_image, range(\n",
    "                    gird_width, width, gird_width), axis=1)\n",
    "                wi1 = wi[::2]\n",
    "                random.shuffle(wi1)\n",
    "                wi2 = wi[1::2]\n",
    "                random.shuffle(wi2)\n",
    "                width_cut_image = np.concatenate(\n",
    "                    (np.concatenate(wi1, axis=1), np.concatenate(wi2, axis=1)), axis=1)\n",
    "                hi = np.split(width_cut_image, range(\n",
    "                    gird_high, high, gird_high), axis=0)\n",
    "                random.shuffle(hi)\n",
    "                hi1 = hi[::2]\n",
    "                random.shuffle(hi1)\n",
    "                hi2 = hi[1::2]\n",
    "                random.shuffle(hi2)\n",
    "                mixer_cut_image = np.concatenate(\n",
    "                    (np.concatenate(hi1, axis=0), np.concatenate(hi2, axis=0)), axis=0)\n",
    "                img_object_thr = mixer_cut_image.astype(\n",
    "                    np.float32) * object_perlin/255.0\n",
    "\n",
    "        beta = torch.rand(1).numpy()[0] * 0.6 + 0.2\n",
    "        augmented_image = image * \\\n",
    "            (1 - object_perlin) + (1 - beta) * \\\n",
    "            img_object_thr + beta * image * (object_perlin)\n",
    "\n",
    "        augmented_image = augmented_image.astype(np.float32)\n",
    "\n",
    "        return augmented_image, msk, np.array([has_anomaly], dtype=np.float32)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \n",
    "        \n",
    "        filename = self.files[index]\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            \n",
    "            gets_anomaly_rand = torch.rand(1).numpy()[0]\n",
    "            \n",
    "            \n",
    "            if gets_anomaly_rand > self.args.synthetic_ratio:\n",
    "                has_anomaly=1\n",
    "                \n",
    "                image = cv2.cvtColor(cv2.imread(filename),cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, dsize=(self.resize_shape[1], self.resize_shape[0]))\n",
    "                \n",
    "                cv2_image=image\n",
    "                thresh_path = self.get_foreground_mvtec(filename)\n",
    "                \n",
    "                print(f'thresh_path: {thresh_path}')\n",
    "                \n",
    "                thresh=cv2.imread(thresh_path,0)\n",
    "                thresh = cv2.resize(thresh,dsize=(self.resize_shape[1], self.resize_shape[0]))\n",
    "\n",
    "                thresh = np.array(thresh).astype(np.float32)/255.0 \n",
    "                image = np.array(image).astype(np.float32)/255.0\n",
    "\n",
    "                anomaly_source_idx = torch.randint(0, len(self.anomaly_source_paths), (1,)).item()\n",
    "                anomaly_path = self.anomaly_source_paths[anomaly_source_idx]\n",
    "                \n",
    "                augmented_image, anomaly_mask, has_anomaly_per  = self.perlin_synthetic(image,thresh,anomaly_path,cv2_image,thresh_path)\n",
    "                \n",
    "                augmented_image = np.transpose(augmented_image, (2, 0, 1))\n",
    "                image = np.transpose(image, (2, 0, 1))\n",
    "                anomaly_mask = np.transpose(anomaly_mask, (2, 0, 1))\n",
    "                \n",
    "                \n",
    "                print(np.shape(augmented_image))\n",
    "                augmented_image=torch.from_numpy(augmented_image)\n",
    "                \n",
    "                print(f'type of img: {type(augmented_image)}, shape of img: {augmented_image.shape} dtype of img: {augmented_image.dtype}')\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                to_pil = transforms.ToPILImage()\n",
    "                pil_image = to_pil(augmented_image)\n",
    "                augmented_image = self.transform_train(pil_image)   \n",
    "                \n",
    "                \n",
    "                return filename, augmented_image, has_anomaly\n",
    "            \n",
    "            \n",
    "            # if there is no anomaly created\n",
    "            else:\n",
    "                has_anomaly = 0\n",
    "                #original resize and transform \n",
    "                # img = self.transform_train(img)      \n",
    "                # with rand augmenter\n",
    "                image = cv2.cvtColor(cv2.imread(filename),cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, dsize=(self.resize_shape[1], self.resize_shape[0]))\n",
    "                # aug = self.randAugmenter()\n",
    "                # image=aug(image=image)\n",
    "                \n",
    "                img = np.array(image).astype(np.float32)/255.0\n",
    "                img=img.transpose(2, 0, 1)\n",
    "                img=torch.from_numpy(img)\n",
    "                \n",
    "                \n",
    "                to_pil = transforms.ToPILImage()\n",
    "                pil_image = to_pil(img)\n",
    "                img = self.transform_train(pil_image)  \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                return filename, pil_image, has_anomaly\n",
    "        \n",
    "        elif self.mode == 'test':\n",
    "            # img = Image.open(filename)\n",
    "            # img = img.convert('RGB')\n",
    "            \n",
    "            image = cv2.cvtColor(cv2.imread(filename),cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, dsize=(self.resize_shape[1], self.resize_shape[0]))\n",
    "            img = np.array(image).astype(np.float32)/255.0\n",
    "            img=img.transpose(2, 0, 1)\n",
    "            img=torch.from_numpy(img)\n",
    "            to_pil = transforms.ToPILImage()\n",
    "            img = to_pil(img)\n",
    "            \n",
    "            print(filename)\n",
    "            \n",
    "            transform_test = self._unalign_transform if self.args.unalign_test else self._align_transform\n",
    "            img_size = (img.size[0], img.size[1])\n",
    "            \n",
    "            \n",
    "            if 'good' in filename:    \n",
    "                ground_truth =Image.new('L',(img_size[0],img_size[1]),0)\n",
    "                img, ground_truth = transform_test(img, ground_truth)\n",
    "                \n",
    "                return filename, img, ground_truth, 0\n",
    "            else:   \n",
    "                # different ground truth schema for mvtec_loco\n",
    "                if self.args.mode=='mvtec_loco':\n",
    "                    ground_truth = Image.open(filename.replace(\"test\", \"ground_truth\").replace(\".png\", \"/000.png\"))                        \n",
    "                if self.args.mode=='mvtec':\n",
    "                    ground_truth = Image.open(filename.replace(\"test\", \"ground_truth\").replace(\".png\", \"_mask.png\"))\n",
    "                \n",
    "                ground_truth = self.resize_transform_loco(ground_truth)  \n",
    "                img, ground_truth = transform_test(img, ground_truth)\n",
    "                \n",
    "                return filename, img, ground_truth, 1\n",
    "\n",
    "\n",
    "DATA_PATH=os.path.join(args.data_root,args.data_category)\n",
    "trainimgs=[DATA_PATH+'/train/good/'+file for file in os.listdir(DATA_PATH+'/train/good')]\n",
    "testpaths=[DATA_PATH+'/train/good/'+file for file in os.listdir(DATA_PATH+'/train/good')]\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(os.path.join('configurations',f'{args.data_set}.json' ), 'r') as file:\n",
    "        dataset_parameters = json.load(file)\n",
    "    setattr(args, 'dataset_parameters', dataset_parameters)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Configuration file for {args.data_set} not found. Proceeding with default parameters.\")\n",
    "    setattr(args, 'dataset_parameters', {})\n",
    "    \n",
    "normal_images, validation_images, sampled_anomalies_for_train, sampled_anomalies_for_val, good_images_test, remaining_anomalies_test = get_paths_mvtec(args,verbose=True)\n",
    "\n",
    "print(normal_images)\n",
    "\n",
    "training_dataset=MVTecSynthAnoDataset(args,DATA_PATH,mode='train',train_paths = normal_images,test_paths = None)\n",
    "train_dataloader = DataLoader(training_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.n_cpu,drop_last=False)\n",
    "\n",
    "\n",
    "dataiter_train = iter(train_dataloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: carpet, normals train:  298, anomalies test: 89, normal test: 28\n",
      "anomalies test total:     {'color': 19, 'cut': 17, 'hole': 17, 'metal_contamination': 17, 'thread': 19}\n",
      "anomalies test sampled:   {'color': 0, 'cut': 0, 'hole': 0, 'metal_contamination': 0, 'thread': 0}\n",
      "anomalies test remaining: {'color': 19, 'cut': 17, 'hole': 17, 'metal_contamination': 17, 'thread': 19}\n",
      "['/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/236.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/182.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/087.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/127.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/148.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/054.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/112.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/237.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/205.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/100.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/081.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/270.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/230.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/036.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/235.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/068.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/184.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/223.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/118.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/001.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/077.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/108.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/173.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/171.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/224.png_aasepkua.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/269.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/110.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/131.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/049.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/226.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/247.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/229.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/076.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/093.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/035.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/032.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/115.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/178.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/133.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/241.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/240.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/109.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/141.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/102.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/074.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/051.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/262.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/246.png_mvgm4js8.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/039.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/129.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/019.png_rsmbbe2x.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/084.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/224.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/172.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/175.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/011.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/190.png_n5ld0z1g.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/069.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/200.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/168.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/012.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/179.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/206.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/124.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/218.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/059.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/061.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/244.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/071.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/104.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/276.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/021.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/222.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/167.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/261.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/220.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/096.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/157.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/082.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/142.png_1od9f22z.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/194.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/013.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/176.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/158.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/085.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/045.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/062.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/019.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/273.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/159.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/275.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/094.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/098.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/058.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/119.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/221.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/075.png_2862l8yn.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/009.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/107.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/228.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/257.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/090.png_tytmohm1.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/065.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/041.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/149.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/028.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/239.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/007.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/245.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/191.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/263.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/163.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/130.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/183.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/106.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/165.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/181.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/231.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/055.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/277.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/174.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/003.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/201.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/203.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/023.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/122.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/160.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/071.png_klleeo18.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/143.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/234.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/016.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/180.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/251.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/155.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/250.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/150.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/248.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/254.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/253.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/243.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/159.png_3eqaxm05.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/188.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/080.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/217.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/219.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/212.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/272.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/162.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/144.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/154.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/274.png_2owdqdl7.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/048.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/213.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/123.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/033.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/137.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/214.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/260.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/101.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/233.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/207.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/025.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/126.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/014.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/199.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/095.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/075.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/256.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/227.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/039.png_aw114cu2.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/187.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/034.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/010.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/145.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/146.png_ni7tejp4.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/249.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/136.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/196.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/151.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/000.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/202.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/037.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/140.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/209.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/116.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/198.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/204.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/152.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/047.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/060.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/063.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/120.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/265.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/044.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/252.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/197.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/153.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/211.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/142.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/132.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/138.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/274.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/056.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/271.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/225.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/004.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/192.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/026.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/022.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/027.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/242.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/170.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/042.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/083.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/208.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/053.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/232.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/161.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/031.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/267.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/241.png_597646z5.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/024.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/089.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/268.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/125.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/114.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/266.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/043.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/255.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/105.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/078.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/050.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/166.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/052.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/117.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/147.png_y4aurzzu.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/057.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/189.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/006.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/111.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/264.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/193.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/029.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/169.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/147.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/134.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/121.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/156.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/005.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/046.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/164.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/103.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/090.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/008.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/038.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/002.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/073.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/030.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/097.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/040.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/161.png_re6t2mqc.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/072.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/064.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/186.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/091.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/092.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/258.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/185.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/088.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/165.png_c8a96h6s.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/079.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/066.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/215.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/238.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/128.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/112.png_wn1urd63.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/086.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/279.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/278.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/139.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/259.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/113.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/099.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/246.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/146.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/177.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/067.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/252.png_8irmj3yv.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/190.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/020.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/216.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/018.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/015.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/135.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/017.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/195.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/210.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/070.png']\n",
      "Number of images in train mode: 298\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/024.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/024.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/028.png\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/003.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/020.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/030.png\n",
      "\n",
      "\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/031.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/030.png\n",
      "texture selectedtexture selectedtexture selected\n",
      "\n",
      "\n",
      "texture selected(3, 256, 256)\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/026.png\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32(3, 256, 256)\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32(3, 256, 256)\n",
      "texture selected\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32texture selected\n",
      "\n",
      "(3, 256, 256)\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.pngtype of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "(3, 256, 256)\n",
      "texture selectedtype of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/008.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/025.png\n",
      "texture selected\n",
      "(3, 256, 256)thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/031.png\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/004.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/002.png\n",
      "\n",
      "texture selected\n",
      "texture selected\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/018.png\n",
      "(3, 256, 256)\n",
      "(3, 256, 256)texture selectedtype of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "(3, 256, 256)(3, 256, 256)\n",
      "\n",
      "texture selectedtype of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selectedthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/009.png\n",
      "\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/022.png\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/012.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/005.png\n",
      "\n",
      "texture selected\n",
      "texture selected\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/025.png\n",
      "texture selectedtexture selected\n",
      "\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/027.png\n",
      "(3, 256, 256)texture selected\n",
      "\n",
      "(3, 256, 256)type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/023.png\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/031.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/020.png\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/015.png\n",
      "texture selected\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/014.png\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "texture selectedthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/007.png\n",
      "\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "(3, 256, 256)(3, 256, 256)\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "texture selected\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "(3, 256, 256)type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/005.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/031.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/017.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/023.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/019.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/002.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/003.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/017.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/022.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/025.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/010.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/032.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/009.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/017.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/018.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/021.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/006.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/022.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/021.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/019.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/021.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/012.pngthresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/028.png\n",
      "\n",
      "texture selected\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/029.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/031.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/026.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/025.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/001.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/030.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/028.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/014.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/020.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/005.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/029.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/007.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/008.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/029.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/020.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/019.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/010.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/019.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/021.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/027.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/010.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/030.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/008.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/012.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/004.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/029.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/027.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/002.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/018.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/003.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/005.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/020.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/031.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/027.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/021.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/032.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/010.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/023.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/030.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/031.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/015.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/017.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/019.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/021.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/027.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/008.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/023.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/004.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/028.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/026.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/026.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/010.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/018.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/013.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/006.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/016.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/011.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/012.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n",
      "thresh_path: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/thresh/018.png\n",
      "texture selected\n",
      "(3, 256, 256)\n",
      "type of img: <class 'torch.Tensor'>, shape of img: torch.Size([3, 256, 256]) dtype of img: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import cv2\n",
    "import glob\n",
    "import imgaug.augmenters as iaa\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# from data.perlin import rand_perlin_2d_np\n",
    "\n",
    "texture_list = ['carpet', 'zipper', 'leather', 'tile', 'wood','grid',\n",
    "                'Class1', 'Class2', 'Class3', 'Class4', 'Class5',\n",
    "                'Class6', 'Class7', 'Class8', 'Class9', 'Class10']\n",
    "\n",
    "category='carpet'\n",
    "\n",
    "anomaly_categories={\n",
    "    \"bottle\": [\"broken_large\", \"broken_small\", \"contamination\"],\n",
    "    \"cable\": [\"bent_wire\", \"cable_swap\", \"combined\", \"cut_inner_insulation\", \"cut_outer_insulation\", \"missing_cable\", \"missing_wire\", \"poke_insulation\"],\n",
    "    \"capsule\": [\"crack\", \"faulty_imprint\", \"poke\", \"scratch\",\"squeeze\"],\n",
    "    \"carpet\": [\"color\", \"cut\", \"hole\", \"metal_contamination\", \"thread\"],\n",
    "    \"grid\": [\"bent\", \"broken\", \"glue\", \"metal_contamination\", \"thread\"],\n",
    "    \"hazelnut\": [\"crack\", \"cut\", \"hole\", \"print\"],\n",
    "    \"leather\": [\"color\", \"cut\", \"fold\", \"glue\", \"poke\"],\n",
    "    \"metal_nut\": [\"bent\", \"color\", \"flip\", \"scratch\"],\n",
    "    \"pill\": [\"color\", \"combined\",\"contamination\", \"crack\", \"faulty_imprint\", \"pill_type\",\"scratch\"],\n",
    "    \"screw\": [\"manipulated_front\", \"scratch_head\", \"scratch_neck\",\"thread_side\", \"thread_top\"],\n",
    "    \"tile\": [\"crack\", \"glue_strip\", \"gray_stroke\", \"oil\",\"rough\"],\n",
    "    \"toothbrush\": [\"defective\"],\n",
    "    \"transistor\": [\"bent_lead\", \"cut_lead\", \"damaged_case\", \"misplaced\"],\n",
    "    \"wood\": [\"color\", \"combined\", \"hole\", \"liquid\", \"scratch\"],\n",
    "    \"zipper\": [\"broken_teeth\", \"combined\",\"fabric_border\", \"fabric_interior\",\"split_teeth\",\"rough\", \"squeezed_teeth\"]}\n",
    "\n",
    "\n",
    "#args = TrainOptions().parse() # surpass kernelerror with this:\n",
    "class TrainOptions:\n",
    "    def __init__(self, category='carpet'):\n",
    "        self.exp_name = \"DEV_DATALOADER\"\n",
    "        self.epoch_start = 0\n",
    "        self.epoch_num = 150\n",
    "        self.factor = 1\n",
    "        self.seed = 233\n",
    "        self.fixed_seed_bool = False\n",
    "        self.test_seed = 400\n",
    "        self.data_ratio = 0.5\n",
    "        self.num_row = 4\n",
    "        self.activation = 'gelu'\n",
    "        self.unalign_test = False\n",
    "        self.data_root = '/home/bule/projects/datasets/mvtec_anomaly_detection/'\n",
    "        self.synthetic_anomaly_root = '/home/bule/projects/datasets/dtd'\n",
    "        self.data_category = category\n",
    "        self.batch_size = 2\n",
    "        self.lr = 1e-4\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 8\n",
    "        self.image_result_dir = 'result_images'\n",
    "        self.model_result_dir = 'saved_models'\n",
    "        self.validation_image_dir = 'validation_images'\n",
    "        self.contamination_rate = 0.0\n",
    "        self.validation= 0.0\n",
    "        self.data_set = 'mvtec'\n",
    "        self.mode = 'mvtec'\n",
    "        self.results_dir = 'results'\n",
    "        self.development = False\n",
    "        self.use_synthetic=True\n",
    "        self.synthetic_ratio=0.0\n",
    "\n",
    "args = TrainOptions()\n",
    "torch.manual_seed(args.seed)\n",
    "    \n",
    "DATA_PATH=os.path.join(args.data_root,args.data_category)\n",
    "\n",
    "try:\n",
    "    with open(os.path.join('configurations',f'{args.data_set}.json' ), 'r') as file:\n",
    "        dataset_parameters = json.load(file)\n",
    "    setattr(args, 'dataset_parameters', dataset_parameters)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Configuration file for {args.data_set} not found. Proceeding with default parameters.\")\n",
    "    setattr(args, 'dataset_parameters', {})\n",
    "    \n",
    "    \n",
    "    \n",
    "normal_images, validation_images, sampled_anomalies_for_train, sampled_anomalies_for_val, good_images_test, remaining_anomalies_test = get_paths_mvtec(args,verbose=True)\n",
    "\n",
    "print(normal_images)\n",
    "\n",
    "training_dataset=MVTecSynthAnoDataset(args,DATA_PATH,mode='train',train_paths = normal_images,test_paths = None)\n",
    "train_dataloader = DataLoader(training_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.n_cpu,drop_last=False)\n",
    "\n",
    "dataiter_train = iter(train_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_number_from_path(path):\n",
    "    # Use regular expression to find the number at the end of the string\n",
    "    match = re.search(r'(\\d+)\\.png$', path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "\n",
    "def random_id(length=8):\n",
    "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "def save_images(img_tensor, save_dir, filenames):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    img_min = img_tensor.min(dim=2, keepdim=True)[0].min(dim=3, keepdim=True)[0]\n",
    "    img_max = img_tensor.max(dim=2, keepdim=True)[0].max(dim=3, keepdim=True)[0]\n",
    "    img_tensor = (img_tensor - img_min) / (img_max - img_min)\n",
    "    \n",
    "    for idx, img in enumerate(img_tensor):\n",
    "        img_rgb = img.permute(1, 2, 0).numpy()\n",
    "        img_bgr = img_rgb[..., ::-1]\n",
    "        random_filename = f\"{extract_number_from_path(filenames[idx])}_{random_id()}.png\"\n",
    "        img_path = os.path.join(save_dir, random_filename)\n",
    "        im = Image.fromarray((img_bgr * 255).astype(np.uint8))\n",
    "        im.save(img_path)\n",
    "        print(f\"Image saved at: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1])\n",
      "('/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/135.png', '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/train/good/128.png')\n",
      "anomalies per batch: {'color': 0, 'cut': 0, 'hole': 0, 'metal_contamination': 0, 'thread': 0}\n",
      "dtype of img: <class 'torch.Tensor'>, shape of img: torch.Size([2, 3, 256, 256]) dtype of img: torch.float32\n",
      "Image saved at: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/synthetic/135_s2tlmjvi.png\n",
      "Image saved at: /home/bule/projects/datasets/mvtec_anomaly_detection/carpet/synthetic/128_ve1l1181.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    images_path, img ,has_ano = next(dataiter_train)\n",
    "    print(has_ano)\n",
    "    print(images_path) # Use next() function to get the next batch\n",
    "    print(f'anomalies per batch: {count_files_by_class(images_path, anomaly_categories[args.data_category])}')\n",
    "    print(f'dtype of img: {type(img)}, shape of img: {img.shape} dtype of img: {img.dtype}')\n",
    "\n",
    "    save_dir = '/home/bule/projects/datasets/mvtec_anomaly_detection/carpet/synthetic'\n",
    "    save_images(img, save_dir, images_path)\n",
    "\n",
    "except ValueError:  # Adjust this based on the structure your dataloader returns\n",
    "    print(\"Error: Adjust the unpacking based on your dataloader's return value\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
