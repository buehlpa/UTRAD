{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List: [50, 20, 40, 10, 30]\n",
      "Indexes sorted by the lowest value: [3, 1, 4, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_sorted_indexes(lst):\n",
    "    indexed_lst = list(enumerate(lst))\n",
    "    sorted_indexed_lst = sorted(indexed_lst, key=lambda x: x[1])\n",
    "    sorted_indexes = [index for index, value in sorted_indexed_lst]\n",
    "    return sorted_indexes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "lst = [50, 20, 40, 10, 30]\n",
    "sorted_indexes = get_sorted_indexes(lst)\n",
    "print(\"List:\", lst)\n",
    "print(\"Indexes sorted by the lowest value:\", sorted_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes sorted by the lowest value: [45, 52, 74, 75, 84, 76, 117, 87, 119, 48, 43, 127, 1, 56, 38, 65, 77, 35, 39, 9, 11, 27, 15, 110, 83, 90, 116, 37, 64, 19, 124, 29, 5, 60, 13, 57, 91, 118, 25, 128, 73, 108, 30, 4, 92, 36, 41, 2, 81, 97, 120, 17, 106, 115, 50, 121, 80, 62, 88, 94, 111, 18, 61, 26, 122, 69, 24, 42, 59, 72, 23, 51, 105, 10, 32, 99, 20, 58, 101, 16, 22, 28, 8, 33, 96, 55, 44, 3, 46, 6, 31, 86, 14, 95, 112, 123, 125, 49, 7, 89, 107, 85, 100, 114, 129, 103, 126, 67, 102, 113, 78, 70, 21, 68, 0, 53, 54, 47, 98, 34, 104, 93, 40, 79, 71, 66, 63, 12, 109, 82]\n"
     ]
    }
   ],
   "source": [
    "def get_sorted_indexes(lst):\n",
    "    # Pair each element with its index\n",
    "    indexed_lst = list(enumerate(lst))\n",
    "    # Sort the list of pairs based on the element values\n",
    "    sorted_indexed_lst = sorted(indexed_lst, key=lambda x: x[1])\n",
    "    # Extract and return the sorted indexes\n",
    "    sorted_indexes = [index for index, value in sorted_indexed_lst]\n",
    "    return sorted_indexes\n",
    "\n",
    "# Provided list\n",
    "lst = [0.021416019648313522, 0.00024144233611878008, 0.0013316604308784008, 0.004344465211033821, \n",
    "       0.0011637076968327165, 0.0007719238637946546, 0.004486515186727047, 0.006980260834097862, \n",
    "       0.0030064983293414116, 0.00036737791378982365, 0.002356914570555091, 0.00037980329943820834, \n",
    "       0.19568675756454468, 0.0008222984033636749, 0.004665277898311615, 0.00043088835082016885, \n",
    "       0.0029613215010613203, 0.001438315026462078, 0.001969587989151478, 0.0005531636415980756, \n",
    "       0.0025351084768772125, 0.015027380548417568, 0.0029668419156223536, 0.0023114033974707127, \n",
    "       0.0021108093205839396, 0.0009671883890405297, 0.0019925241358578205, 0.0003925828787032515, \n",
    "       0.0029741243924945593, 0.0007206216105259955, 0.0011030520545318723, 0.0045439633540809155, \n",
    "       0.0023665083572268486, 0.0031682185363024473, 0.031487613916397095, 0.000348271249094978, \n",
    "       0.0012061435263603926, 0.0005360706127248704, 0.00029135338263586164, 0.00035135907819494605, \n",
    "       0.05142153799533844, 0.0012931987876072526, 0.0021561430767178535, 0.00023195493849925697, \n",
    "       0.0035979312378913164, 0.00013466291420627385, 0.004410024266690016, 0.027080809697508812, \n",
    "       0.00022426724899560213, 0.006788631435483694, 0.0015931401867419481, 0.00233863340690732, \n",
    "       0.00014537552488036454, 0.022114945575594902, 0.023137709125876427, 0.00343411136418581, \n",
    "       0.00026445023831911385, 0.0008598195854574442, 0.0027856517117470503, 0.002170698484405875, \n",
    "       0.0008014058112166822, 0.0019850314129143953, 0.0018400393892079592, 0.08655984699726105, \n",
    "       0.0005363045493140817, 0.0002991177316289395, 0.08322256058454514, 0.011152415536344051, \n",
    "       0.01728084869682789, 0.00206962157972157, 0.014841346070170403, 0.07671210914850235, \n",
    "       0.0022260616533458233, 0.0009978841990232468, 0.0001655022642808035, 0.00017718343588057905, \n",
    "       0.0001889937702799216, 0.0003279408847447485, 0.012935728766024113, 0.07508054375648499, \n",
    "       0.0017835786566138268, 0.0013399451272562146, 1.0, 0.0005047196755185723, \n",
    "       0.0001788665249478072, 0.008486517705023289, 0.004613493103533983, 0.00021763594122603536, \n",
    "       0.0018552019027993083, 0.007358831819146872, 0.0005159948486834764, 0.0009209750569425523, \n",
    "       0.001174535951577127, 0.04253318905830383, 0.0018750583985820413, 0.005510387010872364, \n",
    "       0.0032191884238272905, 0.0013428641250357032, 0.029376214370131493, 0.0024361300747841597, \n",
    "       0.008568333461880684, 0.0027872228529304266, 0.011207940988242626, 0.009573603980243206, \n",
    "       0.03644726797938347, 0.002354297088459134, 0.0015388049650937319, 0.008140410296618938, \n",
    "       0.0010712874354794621, 0.3444184362888336, 0.0004799399757757783, 0.0019434321438893676, \n",
    "       0.005870073102414608, 0.012308170087635517, 0.009252377785742283, 0.0015452259685844183, \n",
    "       0.0005203424370847642, 0.00020778672478627414, 0.0009230530704371631, 0.00021786201978102326, \n",
    "       0.0013931470457464457, 0.00172858324367553, 0.0020324101205915213, 0.005952023435384035, \n",
    "       0.0005720138433389366, 0.0059783728793263435, 0.010502488352358341, 0.00023274385603144765, \n",
    "       0.000992053421214223, 0.009443140588700771]\n",
    "\n",
    "# Get the sorted indexes\n",
    "sorted_indexes = get_sorted_indexes(lst)\n",
    "print(\"Indexes sorted by the lowest value:\", sorted_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models import Create_nets\n",
    "from datasets import get_dataloader\n",
    "from utils.dataloader import get_paths_mvtec\n",
    "from datasets import ImageDataset_mvtec\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pickle\n",
    "from configurations.options import TrainOptions\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import json\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from skimage.measure import label\n",
    "\n",
    "\n",
    "def get_sorted_indexes(lst):\n",
    "    indexed_lst = list(enumerate(lst))\n",
    "    sorted_indexed_lst = sorted(indexed_lst, key=lambda x: x[1])\n",
    "    sorted_indexes = [index for index, value in sorted_indexed_lst]\n",
    "    return sorted_indexes\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = TrainOptions().parse()\n",
    "    \n",
    "    EXPERIMENT_PATH = os.path.join(args.results_dir,args.data_set ,f'contamination_{int(args.contamination_rate*100)}',f'{args.exp_name}-{args.data_category}')\n",
    "        \n",
    "    with open(os.path.join(EXPERIMENT_PATH,'args.log') ,\"a\") as args_log:\n",
    "        for k, v in sorted(vars(args).items()):\n",
    "            print('%s: %s ' % (str(k), str(v)))\n",
    "            args_log.write('%s: %s \\n' % (str(k), str(v)))\n",
    "    print('step')\n",
    "    if args.fixed_seed_bool:\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    else:\n",
    "        torch.manual_seed(int(time.time()))\n",
    "        np.random.seed(int(time.time()))\n",
    "        torch.cuda.manual_seed(int(time.time()))\n",
    "        \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    SAVE_DIR= os.path.join(EXPERIMENT_PATH, args.model_result_dir, 'checkpoint.pth')\n",
    "    SAVE_DIR_REFINED= os.path.join(EXPERIMENT_PATH, args.model_result_dir, 'checkpoint_refined.pth')\n",
    "    \n",
    "    start_epoch = 0\n",
    "    transformer = Create_nets(args)\n",
    "    transformer = transformer.to(device)\n",
    "    transformer.cuda()\n",
    "    optimizer = torch.optim.Adam( transformer.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n",
    "    best_loss = 1e10\n",
    "\n",
    "    backbone = models.resnet18(pretrained=True).to(device)\n",
    "\n",
    "    if os.path.exists(SAVE_DIR):\n",
    "        checkpoint = torch.load(SAVE_DIR)\n",
    "        transformer.load_state_dict(checkpoint['transformer'])\n",
    "        start_epoch = checkpoint['start_epoch']\n",
    "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        del checkpoint\n",
    "\n",
    "    backbone.eval()\n",
    "    outputs = []\n",
    "    # hook adds directly to a list calles outputs if a forward pass is done\n",
    "    def hook(module, input, output):\n",
    "        outputs.append(output)\n",
    "    backbone.layer1[-1].register_forward_hook(hook)\n",
    "    backbone.layer2[-1].register_forward_hook(hook)\n",
    "    backbone.layer3[-1].register_forward_hook(hook)\n",
    "    #backbone.layer4[-1].register_forward_hook(hook)\n",
    "    layer = 3\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_dataloader, valid_loader ,test_dataloader = get_dataloader(args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def embedding_concat(x, y):\n",
    "        B, C1, H1, W1 = x.size()\n",
    "        _, C2, H2, W2 = y.size()\n",
    "        s = int(H1 / H2)\n",
    "        x = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
    "        x = x.view(B, C1, -1, H2, W2)\n",
    "        z = torch.zeros(B, C1 + C2, x.size(2), H2, W2).to(device)\n",
    "        for i in range(x.size(2)):\n",
    "            z[:, :, i, :, :] = torch.cat((x[:, :, i, :, :], y), 1)\n",
    "        z = z.view(B, -1, H2 * W2)\n",
    "        z = F.fold(z, kernel_size=s, output_size=(H1, W1), stride=s)\n",
    "\n",
    "        return z\n",
    "\n",
    "    flag = 0\n",
    "    torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "    for epoch in range(start_epoch, args.epoch_num):\n",
    "        avg_loss = 0\n",
    "        avg_loss_scale = 0\n",
    "        total = 0\n",
    "        transformer.train()\n",
    "        for i,(filename, batch) in enumerate(train_dataloader):\n",
    "            inputs = batch.to(device)\n",
    "            outputs = []\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _ = backbone(inputs)\n",
    "                #outputs = outputs[layer-1]\n",
    "                #outputs = embedding_concat(embedding_concat(embedding_concat(inputs,outputs[0]),outputs[1]),outputs[2])\n",
    "                outputs = embedding_concat(embedding_concat(outputs[0],outputs[1]),outputs[2])\n",
    "        \n",
    "            recon, std = transformer(outputs)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            loss = criterion(recon, outputs)\n",
    "            loss_scale = criterion(std, torch.norm(recon - outputs, p = 2, dim = 1, keepdim = True).detach())\n",
    "            (loss+loss_scale).backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            avg_loss += loss * inputs.size(0)\n",
    "            avg_loss_scale += loss_scale * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "            print((\"\\r[Epoch%d/%d]-[Batch%d/%d]-[Loss:%f]-[Loss_scale:%f]\" %\n",
    "                                                            (epoch+1, args.epoch_num,\n",
    "                                                            i, len(train_dataloader),\n",
    "                                                            avg_loss / total,\n",
    "                                                            avg_loss_scale / total)))\n",
    "\n",
    "\n",
    "        if best_loss > avg_loss and best_loss > loss:\n",
    "            best_loss = avg_loss\n",
    "            state_dict = {\n",
    "                        'start_epoch':epoch,\n",
    "                        #'optimizer':optimizer.state_dict(),\n",
    "                        'transformer':transformer.state_dict(),\n",
    "                        'args':args,\n",
    "                        'best_loss':best_loss\n",
    "                }\n",
    "            torch.save(state_dict, SAVE_DIR)\n",
    "            \n",
    "        ## TODO add validation with data not from testset , loop \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"start evaluation on test set!\")\n",
    "        transformer.eval()\n",
    "        score_map = []\n",
    "        gt_list = []\n",
    "        gt_mask_list = []\n",
    "        for i,(name ,batch, ground_truth, gt) in enumerate(test_dataloader):\n",
    "            with torch.no_grad():\n",
    "                inputs = batch.to(device)\n",
    "                ground_truth = ground_truth.to(device)\n",
    "                outputs = []\n",
    "                _ = backbone(inputs)\n",
    "                outputs = embedding_concat(embedding_concat(outputs[0],outputs[1]),outputs[2])\n",
    "                recon, std = transformer(outputs)\n",
    "                \n",
    "                \n",
    "                \n",
    "                batch_size, channels, width, height = recon.size()\n",
    "                dist = torch.norm(recon - outputs, p = 2, dim = 1, keepdim = True).div(std.abs())\n",
    "                dist = dist.view(batch_size, 1, width, height)\n",
    "                patch_normed_score = []\n",
    "                for j in range(4):\n",
    "                    patch_size = pow(4, j)\n",
    "                    patch_score = F.conv2d(input=dist, \n",
    "                        weight=(torch.ones(1,1,patch_size,patch_size) / (patch_size*patch_size)).to(device), \n",
    "                        bias=None, stride=patch_size, padding=0, dilation=1)\n",
    "                    patch_score = F.avg_pool2d(dist,patch_size,patch_size)\n",
    "                    patch_score = F.interpolate(patch_score, (width,height), mode='bilinear', align_corners=False)\n",
    "                    patch_normed_score.append(patch_score)\n",
    "                score = torch.zeros(batch_size,1,64,64).to(device)\n",
    "                for j in range(4):\n",
    "                    score = embedding_concat(score, patch_normed_score[j])\n",
    "                \n",
    "                score = F.conv2d(input=score, \n",
    "                        weight=torch.tensor([[[[0.0]],[[0.25]],[[0.25]],[[0.25]],[[0.25]]]]).to(device), \n",
    "                        bias=None, stride=1, padding=0, dilation=1)\n",
    "                score = F.interpolate(score, (ground_truth.size(2),ground_truth.size(3)), mode='bilinear', align_corners=False)\n",
    "                heatmap = score.repeat(1,3,1,1)\n",
    "                score_map.append(score.cpu())\n",
    "                gt_mask_list.append(ground_truth.cpu())\n",
    "                gt_list.append(gt)\n",
    "        \n",
    "        score_map = torch.cat(score_map,dim=0)\n",
    "        \n",
    "        gt_mask_list = torch.cat(gt_mask_list,dim=0)\n",
    "        gt_list = torch.cat(gt_list,dim=0)\n",
    "\n",
    "        # Normalization\n",
    "        max_score = score_map.max()\n",
    "        min_score = score_map.min()\n",
    "        scores = (score_map - min_score) / (max_score - min_score)\n",
    "        img_scores = scores.view(scores.size(0),-1).max(dim=1)[0]\n",
    "        \n",
    "        \n",
    "        gt_list = gt_list.numpy()\n",
    "        fpr, tpr, _ = roc_curve(gt_list, img_scores)\n",
    "        img_roc_auc = roc_auc_score(gt_list, img_scores)\n",
    "        print('image ROCAUC: %.3f' % (img_roc_auc))\n",
    "\n",
    "        # calculate per-pixel level ROCAUC\n",
    "        gt_mask = gt_mask_list.numpy().astype('int')\n",
    "        scores = scores.numpy().astype('float32')\n",
    "        fpr, tpr, thresholds = roc_curve(gt_mask.flatten(), scores.flatten()) \n",
    "        per_pixel_rocauc = roc_auc_score(gt_mask.flatten(), scores.flatten()) \n",
    "        print('pixel ROCAUC: %.3f' % (per_pixel_rocauc))\n",
    "        \n",
    "    with open( os.path.join(EXPERIMENT_PATH,'scores_before_refine.pkl'), 'wb') as file:\n",
    "        pickle.dump(img_scores, file)\n",
    "    \n",
    "        \n",
    "    # After training of the initial model   \n",
    "        \n",
    "    if os.path.exists(os.path.join(EXPERIMENT_PATH,\"experiment_paths.json\")) and not args.development:\n",
    "        with open(os.path.join(EXPERIMENT_PATH,\"experiment_paths.json\"), \"r\") as file:\n",
    "            experiment_paths = json.load(file)     \n",
    "    \n",
    "    ## TODO for other datasets currently just for MVTEC\n",
    "    if args.data_set == 'mvtec':\n",
    "        DATA_PATH=os.path.join(args.data_root,args.data_category)\n",
    "        \n",
    "        print(img_scores.tolist())\n",
    "        train_data=experiment_paths['train']\n",
    "        # cut off the images with the highest reconstruction error\n",
    "        refined_idx=get_sorted_indexes(img_scores.tolist())[:int(len(img_scores.tolist())*(1-args.assumed_contamination_rate))]\n",
    "        paths_j = [train_data[idx] for idx in refined_idx]\n",
    "        \n",
    "        with open( os.path.join(EXPERIMENT_PATH,'paths_refined.pkl'), 'wb') as file:\n",
    "            pickle.dump(paths_j, file)\n",
    "        \n",
    "        print(paths_j)\n",
    "        \n",
    "        dataset_refined_simple=ImageDataset_mvtec(args,DATA_PATH,mode='train',train_paths = paths_j, test_paths = None)\n",
    "        train_refined_simple = DataLoader(dataset_refined_simple, batch_size=2,shuffle=True,num_workers=8,drop_last=False)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        print(\"Not implemented for other datasets than MVTEC yet\")\n",
    "        sys.exit()\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Retrain model with refined dataset\n",
    "    for epoch in range(start_epoch, args.epoch_num):\n",
    "        avg_loss = 0\n",
    "        avg_loss_scale = 0\n",
    "        total = 0\n",
    "        transformer.train()\n",
    "        for i,(filename, batch) in enumerate(train_refined_simple):\n",
    "            inputs = batch.to(device)\n",
    "            outputs = []\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _ = backbone(inputs)\n",
    "                #outputs = outputs[layer-1]\n",
    "                #outputs = embedding_concat(embedding_concat(embedding_concat(inputs,outputs[0]),outputs[1]),outputs[2])\n",
    "                outputs = embedding_concat(embedding_concat(outputs[0],outputs[1]),outputs[2])\n",
    "        \n",
    "            recon, std = transformer(outputs)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            loss = criterion(recon, outputs)\n",
    "            loss_scale = criterion(std, torch.norm(recon - outputs, p = 2, dim = 1, keepdim = True).detach())\n",
    "            (loss+loss_scale).backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            avg_loss += loss * inputs.size(0)\n",
    "            avg_loss_scale += loss_scale * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "            print((\"\\r[Epoch%d/%d]-[Batch%d/%d]-[Loss:%f]-[Loss_scale:%f]\" %\n",
    "                                                            (epoch+1, args.epoch_num,\n",
    "                                                            i, len(train_refined_simple),\n",
    "                                                            avg_loss / total,\n",
    "                                                            avg_loss_scale / total)))\n",
    "\n",
    "\n",
    "        if best_loss > avg_loss and best_loss > loss:\n",
    "            best_loss = avg_loss\n",
    "            state_dict = {\n",
    "                        'start_epoch':epoch,\n",
    "                        #'optimizer':optimizer.state_dict(),\n",
    "                        'transformer':transformer.state_dict(),\n",
    "                        'args':args,\n",
    "                        'best_loss':best_loss\n",
    "                }\n",
    "            torch.save(state_dict, SAVE_DIR_REFINED)\n",
    "            \n",
    "        ## TODO add validation with data not from testset , loop \n",
    "        \n",
    "\n",
    "        print(\"start evaluation on test set!\")\n",
    "        transformer.eval()\n",
    "        score_map = []\n",
    "        gt_list = []\n",
    "        gt_mask_list = []\n",
    "        for i,(name ,batch, ground_truth, gt) in enumerate(test_dataloader):\n",
    "            with torch.no_grad():\n",
    "                inputs = batch.to(device)\n",
    "                ground_truth = ground_truth.to(device)\n",
    "                outputs = []\n",
    "                _ = backbone(inputs)\n",
    "                outputs = embedding_concat(embedding_concat(outputs[0],outputs[1]),outputs[2])\n",
    "                recon, std = transformer(outputs)\n",
    "                \n",
    "                batch_size, channels, width, height = recon.size()\n",
    "                dist = torch.norm(recon - outputs, p = 2, dim = 1, keepdim = True).div(std.abs())\n",
    "                dist = dist.view(batch_size, 1, width, height)\n",
    "                patch_normed_score = []\n",
    "                for j in range(4):\n",
    "                    patch_size = pow(4, j)\n",
    "                    patch_score = F.conv2d(input=dist, \n",
    "                        weight=(torch.ones(1,1,patch_size,patch_size) / (patch_size*patch_size)).to(device), \n",
    "                        bias=None, stride=patch_size, padding=0, dilation=1)\n",
    "                    patch_score = F.avg_pool2d(dist,patch_size,patch_size)\n",
    "                    patch_score = F.interpolate(patch_score, (width,height), mode='bilinear', align_corners=False)\n",
    "                    patch_normed_score.append(patch_score)\n",
    "                score = torch.zeros(batch_size,1,64,64).to(device)\n",
    "                for j in range(4):\n",
    "                    score = embedding_concat(score, patch_normed_score[j])\n",
    "                \n",
    "                score = F.conv2d(input=score, \n",
    "                        weight=torch.tensor([[[[0.0]],[[0.25]],[[0.25]],[[0.25]],[[0.25]]]]).to(device), \n",
    "                        bias=None, stride=1, padding=0, dilation=1)\n",
    "                score = F.interpolate(score, (ground_truth.size(2),ground_truth.size(3)), mode='bilinear', align_corners=False)\n",
    "                heatmap = score.repeat(1,3,1,1)\n",
    "                score_map.append(score.cpu())\n",
    "                gt_mask_list.append(ground_truth.cpu())\n",
    "                gt_list.append(gt)\n",
    "        \n",
    "        score_map = torch.cat(score_map,dim=0)\n",
    "        \n",
    "        gt_mask_list = torch.cat(gt_mask_list,dim=0)\n",
    "        gt_list = torch.cat(gt_list,dim=0)\n",
    "\n",
    "        # Normalization\n",
    "        max_score = score_map.max()\n",
    "        min_score = score_map.min()\n",
    "        scores = (score_map - min_score) / (max_score - min_score)\n",
    "        img_scores = scores.view(scores.size(0),-1).max(dim=1)[0]\n",
    "        \n",
    "        \n",
    "        gt_list = gt_list.numpy()\n",
    "        fpr, tpr, _ = roc_curve(gt_list, img_scores)\n",
    "        img_roc_auc = roc_auc_score(gt_list, img_scores)\n",
    "        print('image ROCAUC: %.3f' % (img_roc_auc))\n",
    "\n",
    "        # calculate per-pixel level ROCAUC\n",
    "        gt_mask = gt_mask_list.numpy().astype('int')\n",
    "        scores = scores.numpy().astype('float32')\n",
    "        fpr, tpr, thresholds = roc_curve(gt_mask.flatten(), scores.flatten()) \n",
    "        per_pixel_rocauc = roc_auc_score(gt_mask.flatten(), scores.flatten()) \n",
    "        print('pixel ROCAUC: %.3f' % (per_pixel_rocauc))    \n",
    "        \n",
    "        with open(os.path.join(EXPERIMENT_PATH,'args.log') ,\"a\") as train_log:\n",
    "            train_log.write(\"\\r[Epoch%d]-[Loss:%f]-[Loss_scale:%f]-[image_AUC:%f]-[pixel_AUC:%f]\" %\n",
    "                                                        (epoch+1, avg_loss / total, avg_loss_scale / total, img_roc_auc, per_pixel_rocauc))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect simpleref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/bule/projects/UTRAD/results/mvtec/contamination_10/DEV_SIMPLEREF_18_05_24-screw/experiment_paths.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    experiment_paths = json.load(file)\n",
    "    \n",
    "    file_path = '/home/bule/projects/UTRAD/results/mvtec/contamination_10/DEV_SIMPLEREF_18_05_24-screw/scores_before_refine.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    scores = pickle.load(file)\n",
    "\n",
    "\n",
    "idx= [1 if 'good' in path else 0 for path in experiment_paths['train']]\n",
    "len(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_0 = scores[np.where(np.array(idx) == 0)]\n",
    "score_1 = scores[np.where(np.array(idx) == 1)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(score_0, color='blue', bins=100, alpha=0.7, label='Indicators normals')\n",
    "plt.hist(score_1, color='red', bins=100, alpha=0.7, label='Indicators anos')\n",
    "plt.xlabel('Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2891, 0.4150, 0.3449, 0.3629, 0.3445, 0.3911, 0.3317, 0.3185, 0.3667,\n",
      "        0.3256, 0.3536, 0.3319, 0.3047, 0.3573, 0.4451, 0.3149, 0.3030, 0.7018,\n",
      "        0.6347, 0.3000, 0.3365, 0.3873, 0.3436, 0.4019, 0.3593, 0.3589, 0.3475,\n",
      "        0.3780, 0.3489, 0.3494, 0.3242, 0.4217, 0.3369, 0.3109, 0.3513, 0.3192,\n",
      "        0.3149, 0.3447, 0.3222, 0.3751, 0.3122, 0.4095, 0.5220, 0.3729, 0.4077,\n",
      "        0.5223, 0.3401, 0.4239, 0.3596, 0.3518, 0.3814, 0.3474, 0.3898, 0.3734,\n",
      "        0.2872, 0.4172, 0.3604, 0.4226, 0.3715, 0.4100, 0.4028, 0.4179, 0.4740,\n",
      "        0.3326, 1.0000, 0.4467, 0.4013, 0.4218, 0.4504, 0.3450, 0.3233, 0.3729,\n",
      "        0.4336, 0.3387, 0.3427, 0.3231, 0.4450, 0.3958, 0.4051, 0.3440, 0.3324,\n",
      "        0.4475, 0.8116, 0.5297, 0.3901, 0.4420, 0.4041, 0.4068, 0.4308, 0.4460,\n",
      "        0.3557, 0.3808, 0.3769, 0.4331, 0.3899, 0.3763, 0.3439, 0.4375, 0.4168,\n",
      "        0.3739, 0.3890, 0.3355, 0.4106, 0.3832, 0.3781, 0.3582, 0.3758, 0.3229,\n",
      "        0.3274, 0.3537, 0.4168, 0.4887, 0.8392, 0.4071, 0.4248, 0.4385, 0.4759,\n",
      "        0.2664, 0.3857, 0.4045, 0.4102, 0.3403, 0.4698, 0.4307, 0.4053, 0.5634,\n",
      "        0.4011, 0.5060, 0.4319, 0.3797])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(scores)\n",
    "\n",
    "plt.hist(scores)\n",
    "plt.xlabel('Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
