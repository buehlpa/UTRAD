{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from models import Create_nets\n",
    "from datasets import get_dataloader\n",
    "#from options import TrainOptions\n",
    "from torchvision import models\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from utils.results import *\n",
    "from matplotlib.cm import viridis\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from utils.dataloader import get_paths_mvtec\n",
    "from datasets import ImageDataset_mvtec\n",
    "from torch.utils.data import DataLoader\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "class TrainOptions:\n",
    "    def __init__(self):\n",
    "        self.exp_name = \"DEV_USDR\"\n",
    "        self.epoch_start = 0\n",
    "        self.epoch_num = 1\n",
    "        self.factor = 1\n",
    "        self.seed = 233\n",
    "        self.fixed_seed_bool = True\n",
    "        self.test_seed = 400\n",
    "        self.num_row = 4\n",
    "        self.activation = 'gelu'\n",
    "        self.unalign_test = False\n",
    "        self.data_root = '/home/bule/projects/datasets/mvtec_anomaly_detection/'\n",
    "        self.data_category = \"cable\"\n",
    "        self.data_set = \"mvtec\"\n",
    "        self.batch_size = 2\n",
    "        self.lr = 1e-4\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 8\n",
    "        self.results_dir = 'results'\n",
    "        self.image_result_dir = 'result_images'\n",
    "        self.model_result_dir = 'saved_models'\n",
    "        self.validation_image_dir = 'validation_images'\n",
    "        self.contamination_rate = 0.1\n",
    "        self.assumed_contamination_rate = 0.1\n",
    "        self.mode = 'mvtec'\n",
    "        self.development = False\n",
    "        self.parser = None\n",
    "        self.initialized = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f75d3f157d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainOptions()\n",
    "os.makedirs(args.results_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.results_dir, args.data_set, f'contamination_{int(args.contamination_rate*100)}', f'{args.exp_name}-{args.data_category}', args.image_result_dir), exist_ok=True)\n",
    "os.makedirs(os.path.join(args.results_dir, args.data_set, f'contamination_{int(args.contamination_rate*100)}', f'{args.exp_name}-{args.data_category}', args.model_result_dir), exist_ok=True)\n",
    "\n",
    "try:\n",
    "    with open(os.path.join('configurations', f'{args.data_set}.json'), 'r') as file:\n",
    "        dataset_parameters = json.load(file)\n",
    "    setattr(args, 'dataset_parameters', dataset_parameters)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Configuration file for {args.data_set} not found. Proceeding with default parameters.\")\n",
    "    setattr(args, 'dataset_parameters', {})\n",
    "    \n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: cable, normals train:  224, anomalies test: 92, normal test: 58\n",
      "anomalies test total:     {'bent_wire': 13, 'cable_swap': 12, 'combined': 11, 'cut_inner_insulation': 14, 'cut_outer_insulation': 10, 'missing_cable': 12, 'missing_wire': 10, 'poke_insulation': 10}\n",
      "anomalies test sampled:   {'bent_wire': 3, 'cable_swap': 2, 'combined': 2, 'cut_inner_insulation': 3, 'cut_outer_insulation': 2, 'missing_cable': 2, 'missing_wire': 2, 'poke_insulation': 2}\n",
      "anomalies test remaining: {'bent_wire': 10, 'cable_swap': 10, 'combined': 9, 'cut_inner_insulation': 11, 'cut_outer_insulation': 8, 'missing_cable': 10, 'missing_wire': 8, 'poke_insulation': 8}\n"
     ]
    }
   ],
   "source": [
    "normal_images, validation_images, sampled_anomalies_for_train, sampled_anomalies_for_val, good_images_test, remaining_anomalies_test = get_paths_mvtec(args,verbose=True)\n",
    "DATA_PATH=os.path.join(args.data_root,args.data_category)\n",
    "# combine good and anomalies\n",
    "train_data=normal_images+sampled_anomalies_for_train\n",
    "\n",
    "train_dataloader = DataLoader(ImageDataset_mvtec(args,DATA_PATH,mode='train',train_paths = train_data,test_paths = None),\n",
    "                                                batch_size=args.batch_size,shuffle=True,num_workers=args.n_cpu,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "train_sets:  5\n",
      "window_size:  120\n",
      "[Epoch1/1]-[Batch0/60]-[Loss:0.343237]-[Loss_scale:155.271790]\n",
      "[Epoch1/1]-[Batch1/60]-[Loss:0.296613]-[Loss_scale:134.087143]\n",
      "[Epoch1/1]-[Batch2/60]-[Loss:0.259177]-[Loss_scale:117.110855]\n",
      "[Epoch1/1]-[Batch3/60]-[Loss:0.229817]-[Loss_scale:103.822578]\n",
      "[Epoch1/1]-[Batch4/60]-[Loss:0.206021]-[Loss_scale:93.070236]\n",
      "[Epoch1/1]-[Batch5/60]-[Loss:0.187784]-[Loss_scale:84.838913]\n",
      "[Epoch1/1]-[Batch6/60]-[Loss:0.173962]-[Loss_scale:78.603844]\n",
      "[Epoch1/1]-[Batch7/60]-[Loss:0.163030]-[Loss_scale:73.671822]\n",
      "[Epoch1/1]-[Batch8/60]-[Loss:0.154652]-[Loss_scale:69.890991]\n",
      "[Epoch1/1]-[Batch9/60]-[Loss:0.147676]-[Loss_scale:66.741508]\n",
      "[Epoch1/1]-[Batch10/60]-[Loss:0.141923]-[Loss_scale:64.142860]\n",
      "[Epoch1/1]-[Batch11/60]-[Loss:0.137115]-[Loss_scale:61.969727]\n",
      "[Epoch1/1]-[Batch12/60]-[Loss:0.132997]-[Loss_scale:60.106766]\n",
      "[Epoch1/1]-[Batch13/60]-[Loss:0.129406]-[Loss_scale:58.480824]\n",
      "[Epoch1/1]-[Batch14/60]-[Loss:0.126265]-[Loss_scale:57.057125]\n",
      "[Epoch1/1]-[Batch15/60]-[Loss:0.123558]-[Loss_scale:55.827858]\n",
      "[Epoch1/1]-[Batch16/60]-[Loss:0.121107]-[Loss_scale:54.714397]\n",
      "[Epoch1/1]-[Batch17/60]-[Loss:0.118912]-[Loss_scale:53.715164]\n",
      "[Epoch1/1]-[Batch18/60]-[Loss:0.117060]-[Loss_scale:52.870647]\n",
      "[Epoch1/1]-[Batch19/60]-[Loss:0.115371]-[Loss_scale:52.098511]\n",
      "[Epoch1/1]-[Batch20/60]-[Loss:0.113751]-[Loss_scale:51.357098]\n",
      "[Epoch1/1]-[Batch21/60]-[Loss:0.112370]-[Loss_scale:50.723408]\n",
      "[Epoch1/1]-[Batch22/60]-[Loss:0.111031]-[Loss_scale:50.108555]\n",
      "[Epoch1/1]-[Batch23/60]-[Loss:0.109904]-[Loss_scale:49.588226]\n",
      "[Epoch1/1]-[Batch24/60]-[Loss:0.108811]-[Loss_scale:49.083504]\n",
      "[Epoch1/1]-[Batch25/60]-[Loss:0.107772]-[Loss_scale:48.602921]\n",
      "[Epoch1/1]-[Batch26/60]-[Loss:0.106763]-[Loss_scale:48.135902]\n",
      "[Epoch1/1]-[Batch27/60]-[Loss:0.105868]-[Loss_scale:47.719769]\n",
      "[Epoch1/1]-[Batch28/60]-[Loss:0.104965]-[Loss_scale:47.299870]\n",
      "[Epoch1/1]-[Batch29/60]-[Loss:0.104229]-[Loss_scale:46.954914]\n",
      "[Epoch1/1]-[Batch30/60]-[Loss:0.103448]-[Loss_scale:46.589668]\n",
      "[Epoch1/1]-[Batch31/60]-[Loss:0.102674]-[Loss_scale:46.227764]\n",
      "[Epoch1/1]-[Batch32/60]-[Loss:0.102042]-[Loss_scale:45.929173]\n",
      "[Epoch1/1]-[Batch33/60]-[Loss:0.101352]-[Loss_scale:45.605137]\n",
      "[Epoch1/1]-[Batch34/60]-[Loss:0.100780]-[Loss_scale:45.333405]\n",
      "[Epoch1/1]-[Batch35/60]-[Loss:0.100200]-[Loss_scale:45.057972]\n",
      "[Epoch1/1]-[Batch36/60]-[Loss:0.099712]-[Loss_scale:44.824005]\n",
      "[Epoch1/1]-[Batch37/60]-[Loss:0.099181]-[Loss_scale:44.570126]\n",
      "[Epoch1/1]-[Batch38/60]-[Loss:0.098650]-[Loss_scale:44.316853]\n",
      "[Epoch1/1]-[Batch39/60]-[Loss:0.098191]-[Loss_scale:44.095818]\n",
      "[Epoch1/1]-[Batch40/60]-[Loss:0.097734]-[Loss_scale:43.875549]\n",
      "[Epoch1/1]-[Batch41/60]-[Loss:0.097332]-[Loss_scale:43.680111]\n",
      "[Epoch1/1]-[Batch42/60]-[Loss:0.096885]-[Loss_scale:43.464375]\n",
      "[Epoch1/1]-[Batch43/60]-[Loss:0.096494]-[Loss_scale:43.273380]\n",
      "[Epoch1/1]-[Batch44/60]-[Loss:0.096066]-[Loss_scale:43.066780]\n",
      "[Epoch1/1]-[Batch45/60]-[Loss:0.095682]-[Loss_scale:42.879280]\n",
      "[Epoch1/1]-[Batch46/60]-[Loss:0.095333]-[Loss_scale:42.706963]\n",
      "[Epoch1/1]-[Batch47/60]-[Loss:0.095033]-[Loss_scale:42.557007]\n",
      "[Epoch1/1]-[Batch48/60]-[Loss:0.094702]-[Loss_scale:42.393135]\n",
      "[Epoch1/1]-[Batch49/60]-[Loss:0.094347]-[Loss_scale:42.218826]\n",
      "[Epoch1/1]-[Batch50/60]-[Loss:0.094024]-[Loss_scale:42.058762]\n",
      "[Epoch1/1]-[Batch51/60]-[Loss:0.093681]-[Loss_scale:41.889751]\n",
      "[Epoch1/1]-[Batch52/60]-[Loss:0.093393]-[Loss_scale:41.745426]\n",
      "[Epoch1/1]-[Batch53/60]-[Loss:0.093090]-[Loss_scale:41.594376]\n",
      "[Epoch1/1]-[Batch54/60]-[Loss:0.092817]-[Loss_scale:41.455990]\n",
      "[Epoch1/1]-[Batch55/60]-[Loss:0.092523]-[Loss_scale:41.308315]\n",
      "[Epoch1/1]-[Batch56/60]-[Loss:0.092269]-[Loss_scale:41.179066]\n",
      "[Epoch1/1]-[Batch57/60]-[Loss:0.091986]-[Loss_scale:41.037144]\n",
      "[Epoch1/1]-[Batch58/60]-[Loss:0.091738]-[Loss_scale:40.909904]\n",
      "[Epoch1/1]-[Batch59/60]-[Loss:0.091467]-[Loss_scale:40.771698]\n",
      "[Epoch1/1]-[Batch0/60]-[Loss:0.363746]-[Loss_scale:164.211197]\n",
      "[Epoch1/1]-[Batch1/60]-[Loss:0.316594]-[Loss_scale:142.803207]\n",
      "[Epoch1/1]-[Batch2/60]-[Loss:0.279258]-[Loss_scale:125.875046]\n",
      "[Epoch1/1]-[Batch3/60]-[Loss:0.247927]-[Loss_scale:111.702988]\n",
      "[Epoch1/1]-[Batch4/60]-[Loss:0.222745]-[Loss_scale:100.330940]\n",
      "[Epoch1/1]-[Batch5/60]-[Loss:0.202680]-[Loss_scale:91.282639]\n",
      "[Epoch1/1]-[Batch6/60]-[Loss:0.187015]-[Loss_scale:84.223877]\n",
      "[Epoch1/1]-[Batch7/60]-[Loss:0.174484]-[Loss_scale:78.580383]\n",
      "[Epoch1/1]-[Batch8/60]-[Loss:0.164701]-[Loss_scale:74.174538]\n",
      "[Epoch1/1]-[Batch9/60]-[Loss:0.156912]-[Loss_scale:70.665245]\n",
      "[Epoch1/1]-[Batch10/60]-[Loss:0.150199]-[Loss_scale:67.640396]\n",
      "[Epoch1/1]-[Batch11/60]-[Loss:0.144767]-[Loss_scale:65.190956]\n",
      "[Epoch1/1]-[Batch12/60]-[Loss:0.140021]-[Loss_scale:63.049648]\n",
      "[Epoch1/1]-[Batch13/60]-[Loss:0.135920]-[Loss_scale:61.197773]\n",
      "[Epoch1/1]-[Batch14/60]-[Loss:0.132312]-[Loss_scale:59.566849]\n",
      "[Epoch1/1]-[Batch15/60]-[Loss:0.129127]-[Loss_scale:58.125916]\n",
      "[Epoch1/1]-[Batch16/60]-[Loss:0.126295]-[Loss_scale:56.843025]\n",
      "[Epoch1/1]-[Batch17/60]-[Loss:0.123795]-[Loss_scale:55.709080]\n",
      "[Epoch1/1]-[Batch18/60]-[Loss:0.121659]-[Loss_scale:54.738682]\n",
      "[Epoch1/1]-[Batch19/60]-[Loss:0.119707]-[Loss_scale:53.850544]\n",
      "[Epoch1/1]-[Batch20/60]-[Loss:0.117837]-[Loss_scale:52.998810]\n",
      "[Epoch1/1]-[Batch21/60]-[Loss:0.116167]-[Loss_scale:52.236763]\n",
      "[Epoch1/1]-[Batch22/60]-[Loss:0.114636]-[Loss_scale:51.537132]\n",
      "[Epoch1/1]-[Batch23/60]-[Loss:0.113253]-[Loss_scale:50.903973]\n",
      "[Epoch1/1]-[Batch24/60]-[Loss:0.112052]-[Loss_scale:50.351986]\n",
      "[Epoch1/1]-[Batch25/60]-[Loss:0.110838]-[Loss_scale:49.794041]\n",
      "[Epoch1/1]-[Batch26/60]-[Loss:0.109719]-[Loss_scale:49.278603]\n",
      "[Epoch1/1]-[Batch27/60]-[Loss:0.108670]-[Loss_scale:48.794552]\n",
      "[Epoch1/1]-[Batch28/60]-[Loss:0.107755]-[Loss_scale:48.370644]\n",
      "[Epoch1/1]-[Batch29/60]-[Loss:0.106930]-[Loss_scale:47.986778]\n",
      "[Epoch1/1]-[Batch30/60]-[Loss:0.106123]-[Loss_scale:47.611343]\n",
      "[Epoch1/1]-[Batch31/60]-[Loss:0.105351]-[Loss_scale:47.251293]\n",
      "[Epoch1/1]-[Batch32/60]-[Loss:0.104634]-[Loss_scale:46.915905]\n",
      "[Epoch1/1]-[Batch33/60]-[Loss:0.103935]-[Loss_scale:46.588425]\n",
      "[Epoch1/1]-[Batch34/60]-[Loss:0.103273]-[Loss_scale:46.277500]\n",
      "[Epoch1/1]-[Batch35/60]-[Loss:0.102631]-[Loss_scale:45.975887]\n",
      "[Epoch1/1]-[Batch36/60]-[Loss:0.102023]-[Loss_scale:45.689529]\n",
      "[Epoch1/1]-[Batch37/60]-[Loss:0.101539]-[Loss_scale:45.457985]\n",
      "[Epoch1/1]-[Batch38/60]-[Loss:0.100978]-[Loss_scale:45.191936]\n",
      "[Epoch1/1]-[Batch39/60]-[Loss:0.100473]-[Loss_scale:44.951210]\n",
      "[Epoch1/1]-[Batch40/60]-[Loss:0.099993]-[Loss_scale:44.721439]\n",
      "[Epoch1/1]-[Batch41/60]-[Loss:0.099529]-[Loss_scale:44.499245]\n",
      "[Epoch1/1]-[Batch42/60]-[Loss:0.099102]-[Loss_scale:44.292843]\n",
      "[Epoch1/1]-[Batch43/60]-[Loss:0.098632]-[Loss_scale:44.067726]\n",
      "[Epoch1/1]-[Batch44/60]-[Loss:0.098184]-[Loss_scale:43.852398]\n",
      "[Epoch1/1]-[Batch45/60]-[Loss:0.097769]-[Loss_scale:43.651691]\n",
      "[Epoch1/1]-[Batch46/60]-[Loss:0.097362]-[Loss_scale:43.454941]\n",
      "[Epoch1/1]-[Batch47/60]-[Loss:0.097009]-[Loss_scale:43.282223]\n",
      "[Epoch1/1]-[Batch48/60]-[Loss:0.096609]-[Loss_scale:43.088272]\n",
      "[Epoch1/1]-[Batch49/60]-[Loss:0.096219]-[Loss_scale:42.899414]\n",
      "[Epoch1/1]-[Batch50/60]-[Loss:0.095909]-[Loss_scale:42.744488]\n",
      "[Epoch1/1]-[Batch51/60]-[Loss:0.095561]-[Loss_scale:42.573318]\n",
      "[Epoch1/1]-[Batch52/60]-[Loss:0.095246]-[Loss_scale:42.417225]\n",
      "[Epoch1/1]-[Batch53/60]-[Loss:0.094935]-[Loss_scale:42.262718]\n",
      "[Epoch1/1]-[Batch54/60]-[Loss:0.094617]-[Loss_scale:42.106091]\n",
      "[Epoch1/1]-[Batch55/60]-[Loss:0.094331]-[Loss_scale:41.963310]\n",
      "[Epoch1/1]-[Batch56/60]-[Loss:0.093985]-[Loss_scale:41.793793]\n",
      "[Epoch1/1]-[Batch57/60]-[Loss:0.093705]-[Loss_scale:41.653080]\n",
      "[Epoch1/1]-[Batch58/60]-[Loss:0.093439]-[Loss_scale:41.518764]\n",
      "[Epoch1/1]-[Batch59/60]-[Loss:0.093149]-[Loss_scale:41.373878]\n",
      "[Epoch1/1]-[Batch0/60]-[Loss:0.364313]-[Loss_scale:162.184174]\n",
      "[Epoch1/1]-[Batch1/60]-[Loss:0.314987]-[Loss_scale:139.988617]\n",
      "[Epoch1/1]-[Batch2/60]-[Loss:0.275229]-[Loss_scale:122.145370]\n",
      "[Epoch1/1]-[Batch3/60]-[Loss:0.244339]-[Loss_scale:108.312141]\n",
      "[Epoch1/1]-[Batch4/60]-[Loss:0.218993]-[Loss_scale:96.985741]\n",
      "[Epoch1/1]-[Batch5/60]-[Loss:0.199562]-[Loss_scale:88.314285]\n",
      "[Epoch1/1]-[Batch6/60]-[Loss:0.184390]-[Loss_scale:81.550774]\n",
      "[Epoch1/1]-[Batch7/60]-[Loss:0.172715]-[Loss_scale:76.347946]\n",
      "[Epoch1/1]-[Batch8/60]-[Loss:0.163138]-[Loss_scale:72.081741]\n",
      "[Epoch1/1]-[Batch9/60]-[Loss:0.155403]-[Loss_scale:68.635620]\n",
      "[Epoch1/1]-[Batch10/60]-[Loss:0.149099]-[Loss_scale:65.824905]\n",
      "[Epoch1/1]-[Batch11/60]-[Loss:0.143850]-[Loss_scale:63.482861]\n",
      "[Epoch1/1]-[Batch12/60]-[Loss:0.139279]-[Loss_scale:61.442413]\n",
      "[Epoch1/1]-[Batch13/60]-[Loss:0.135375]-[Loss_scale:59.698345]\n",
      "[Epoch1/1]-[Batch14/60]-[Loss:0.131927]-[Loss_scale:58.156147]\n",
      "[Epoch1/1]-[Batch15/60]-[Loss:0.128879]-[Loss_scale:56.791855]\n",
      "[Epoch1/1]-[Batch16/60]-[Loss:0.126285]-[Loss_scale:55.629356]\n",
      "[Epoch1/1]-[Batch17/60]-[Loss:0.123871]-[Loss_scale:54.545815]\n",
      "[Epoch1/1]-[Batch18/60]-[Loss:0.121670]-[Loss_scale:53.557178]\n",
      "[Epoch1/1]-[Batch19/60]-[Loss:0.119722]-[Loss_scale:52.680939]\n",
      "[Epoch1/1]-[Batch20/60]-[Loss:0.117959]-[Loss_scale:51.886318]\n",
      "[Epoch1/1]-[Batch21/60]-[Loss:0.116164]-[Loss_scale:51.077633]\n",
      "[Epoch1/1]-[Batch22/60]-[Loss:0.114720]-[Loss_scale:50.424210]\n",
      "[Epoch1/1]-[Batch23/60]-[Loss:0.113425]-[Loss_scale:49.836609]\n",
      "[Epoch1/1]-[Batch24/60]-[Loss:0.112134]-[Loss_scale:49.250950]\n",
      "[Epoch1/1]-[Batch25/60]-[Loss:0.110901]-[Loss_scale:48.691238]\n",
      "[Epoch1/1]-[Batch26/60]-[Loss:0.109779]-[Loss_scale:48.180218]\n",
      "[Epoch1/1]-[Batch27/60]-[Loss:0.108716]-[Loss_scale:47.695412]\n",
      "[Epoch1/1]-[Batch28/60]-[Loss:0.107701]-[Loss_scale:47.232170]\n",
      "[Epoch1/1]-[Batch29/60]-[Loss:0.106825]-[Loss_scale:46.830204]\n",
      "[Epoch1/1]-[Batch30/60]-[Loss:0.106047]-[Loss_scale:46.471355]\n",
      "[Epoch1/1]-[Batch31/60]-[Loss:0.105312]-[Loss_scale:46.131718]\n",
      "[Epoch1/1]-[Batch32/60]-[Loss:0.104540]-[Loss_scale:45.775608]\n",
      "[Epoch1/1]-[Batch33/60]-[Loss:0.103834]-[Loss_scale:45.448837]\n",
      "[Epoch1/1]-[Batch34/60]-[Loss:0.103144]-[Loss_scale:45.129482]\n",
      "[Epoch1/1]-[Batch35/60]-[Loss:0.102500]-[Loss_scale:44.829823]\n",
      "[Epoch1/1]-[Batch36/60]-[Loss:0.101909]-[Loss_scale:44.553638]\n",
      "[Epoch1/1]-[Batch37/60]-[Loss:0.101378]-[Loss_scale:44.303867]\n",
      "[Epoch1/1]-[Batch38/60]-[Loss:0.100794]-[Loss_scale:44.031593]\n",
      "[Epoch1/1]-[Batch39/60]-[Loss:0.100282]-[Loss_scale:43.790112]\n",
      "[Epoch1/1]-[Batch40/60]-[Loss:0.099734]-[Loss_scale:43.533375]\n",
      "[Epoch1/1]-[Batch41/60]-[Loss:0.099278]-[Loss_scale:43.316696]\n",
      "[Epoch1/1]-[Batch42/60]-[Loss:0.098817]-[Loss_scale:43.097900]\n",
      "[Epoch1/1]-[Batch43/60]-[Loss:0.098382]-[Loss_scale:42.890190]\n",
      "[Epoch1/1]-[Batch44/60]-[Loss:0.097966]-[Loss_scale:42.691101]\n",
      "[Epoch1/1]-[Batch45/60]-[Loss:0.097570]-[Loss_scale:42.500504]\n",
      "[Epoch1/1]-[Batch46/60]-[Loss:0.097239]-[Loss_scale:42.339081]\n",
      "[Epoch1/1]-[Batch47/60]-[Loss:0.096863]-[Loss_scale:42.157406]\n",
      "[Epoch1/1]-[Batch48/60]-[Loss:0.096479]-[Loss_scale:41.972466]\n",
      "[Epoch1/1]-[Batch49/60]-[Loss:0.096097]-[Loss_scale:41.788414]\n",
      "[Epoch1/1]-[Batch50/60]-[Loss:0.095784]-[Loss_scale:41.634171]\n",
      "[Epoch1/1]-[Batch51/60]-[Loss:0.095437]-[Loss_scale:41.465252]\n",
      "[Epoch1/1]-[Batch52/60]-[Loss:0.095106]-[Loss_scale:41.303814]\n",
      "[Epoch1/1]-[Batch53/60]-[Loss:0.094788]-[Loss_scale:41.147995]\n",
      "[Epoch1/1]-[Batch54/60]-[Loss:0.094485]-[Loss_scale:40.998127]\n",
      "[Epoch1/1]-[Batch55/60]-[Loss:0.094147]-[Loss_scale:40.833630]\n",
      "[Epoch1/1]-[Batch56/60]-[Loss:0.093824]-[Loss_scale:40.676128]\n",
      "[Epoch1/1]-[Batch57/60]-[Loss:0.093543]-[Loss_scale:40.535854]\n",
      "[Epoch1/1]-[Batch58/60]-[Loss:0.093253]-[Loss_scale:40.391914]\n",
      "[Epoch1/1]-[Batch59/60]-[Loss:0.092987]-[Loss_scale:40.259071]\n",
      "[Epoch1/1]-[Batch0/60]-[Loss:0.352365]-[Loss_scale:157.406830]\n",
      "[Epoch1/1]-[Batch1/60]-[Loss:0.307480]-[Loss_scale:137.149094]\n",
      "[Epoch1/1]-[Batch2/60]-[Loss:0.270562]-[Loss_scale:120.530174]\n",
      "[Epoch1/1]-[Batch3/60]-[Loss:0.240181]-[Loss_scale:106.888199]\n",
      "[Epoch1/1]-[Batch4/60]-[Loss:0.216472]-[Loss_scale:96.265457]\n",
      "[Epoch1/1]-[Batch5/60]-[Loss:0.197408]-[Loss_scale:87.740158]\n",
      "[Epoch1/1]-[Batch6/60]-[Loss:0.182370]-[Loss_scale:81.022018]\n",
      "[Epoch1/1]-[Batch7/60]-[Loss:0.170332]-[Loss_scale:75.648033]\n",
      "[Epoch1/1]-[Batch8/60]-[Loss:0.160860]-[Loss_scale:71.420265]\n",
      "[Epoch1/1]-[Batch9/60]-[Loss:0.153556]-[Loss_scale:68.159241]\n",
      "[Epoch1/1]-[Batch10/60]-[Loss:0.147388]-[Loss_scale:65.403778]\n",
      "[Epoch1/1]-[Batch11/60]-[Loss:0.142121]-[Loss_scale:63.049751]\n",
      "[Epoch1/1]-[Batch12/60]-[Loss:0.137613]-[Loss_scale:61.033257]\n",
      "[Epoch1/1]-[Batch13/60]-[Loss:0.133685]-[Loss_scale:59.275665]\n",
      "[Epoch1/1]-[Batch14/60]-[Loss:0.130296]-[Loss_scale:57.757462]\n",
      "[Epoch1/1]-[Batch15/60]-[Loss:0.127272]-[Loss_scale:56.401222]\n",
      "[Epoch1/1]-[Batch16/60]-[Loss:0.124583]-[Loss_scale:55.193840]\n",
      "[Epoch1/1]-[Batch17/60]-[Loss:0.122121]-[Loss_scale:54.087822]\n",
      "[Epoch1/1]-[Batch18/60]-[Loss:0.119915]-[Loss_scale:53.095284]\n",
      "[Epoch1/1]-[Batch19/60]-[Loss:0.118096]-[Loss_scale:52.274384]\n",
      "[Epoch1/1]-[Batch20/60]-[Loss:0.116437]-[Loss_scale:51.524925]\n",
      "[Epoch1/1]-[Batch21/60]-[Loss:0.114871]-[Loss_scale:50.816540]\n",
      "[Epoch1/1]-[Batch22/60]-[Loss:0.113535]-[Loss_scale:50.210163]\n",
      "[Epoch1/1]-[Batch23/60]-[Loss:0.112190]-[Loss_scale:49.599556]\n",
      "[Epoch1/1]-[Batch24/60]-[Loss:0.110867]-[Loss_scale:48.999073]\n",
      "[Epoch1/1]-[Batch25/60]-[Loss:0.109664]-[Loss_scale:48.452011]\n",
      "[Epoch1/1]-[Batch26/60]-[Loss:0.108631]-[Loss_scale:47.979679]\n",
      "[Epoch1/1]-[Batch27/60]-[Loss:0.107679]-[Loss_scale:47.543537]\n",
      "[Epoch1/1]-[Batch28/60]-[Loss:0.106746]-[Loss_scale:47.115490]\n",
      "[Epoch1/1]-[Batch29/60]-[Loss:0.105872]-[Loss_scale:46.713707]\n",
      "[Epoch1/1]-[Batch30/60]-[Loss:0.105137]-[Loss_scale:46.373283]\n",
      "[Epoch1/1]-[Batch31/60]-[Loss:0.104416]-[Loss_scale:46.039085]\n",
      "[Epoch1/1]-[Batch32/60]-[Loss:0.103647]-[Loss_scale:45.684135]\n",
      "[Epoch1/1]-[Batch33/60]-[Loss:0.103016]-[Loss_scale:45.389866]\n",
      "[Epoch1/1]-[Batch34/60]-[Loss:0.102378]-[Loss_scale:45.092758]\n",
      "[Epoch1/1]-[Batch35/60]-[Loss:0.101811]-[Loss_scale:44.826607]\n",
      "[Epoch1/1]-[Batch36/60]-[Loss:0.101229]-[Loss_scale:44.553715]\n",
      "[Epoch1/1]-[Batch37/60]-[Loss:0.100610]-[Loss_scale:44.265118]\n",
      "[Epoch1/1]-[Batch38/60]-[Loss:0.100117]-[Loss_scale:44.032215]\n",
      "[Epoch1/1]-[Batch39/60]-[Loss:0.099637]-[Loss_scale:43.804913]\n",
      "[Epoch1/1]-[Batch40/60]-[Loss:0.099148]-[Loss_scale:43.572983]\n",
      "[Epoch1/1]-[Batch41/60]-[Loss:0.098657]-[Loss_scale:43.342186]\n",
      "[Epoch1/1]-[Batch42/60]-[Loss:0.098190]-[Loss_scale:43.120010]\n",
      "[Epoch1/1]-[Batch43/60]-[Loss:0.097740]-[Loss_scale:42.905548]\n",
      "[Epoch1/1]-[Batch44/60]-[Loss:0.097344]-[Loss_scale:42.715145]\n",
      "[Epoch1/1]-[Batch45/60]-[Loss:0.096928]-[Loss_scale:42.516285]\n",
      "[Epoch1/1]-[Batch46/60]-[Loss:0.096535]-[Loss_scale:42.327690]\n",
      "[Epoch1/1]-[Batch47/60]-[Loss:0.096131]-[Loss_scale:42.134041]\n",
      "[Epoch1/1]-[Batch48/60]-[Loss:0.095730]-[Loss_scale:41.942009]\n",
      "[Epoch1/1]-[Batch49/60]-[Loss:0.095381]-[Loss_scale:41.772289]\n",
      "[Epoch1/1]-[Batch50/60]-[Loss:0.095057]-[Loss_scale:41.614735]\n",
      "[Epoch1/1]-[Batch51/60]-[Loss:0.094771]-[Loss_scale:41.472893]\n",
      "[Epoch1/1]-[Batch52/60]-[Loss:0.094428]-[Loss_scale:41.306343]\n",
      "[Epoch1/1]-[Batch53/60]-[Loss:0.094113]-[Loss_scale:41.151653]\n",
      "[Epoch1/1]-[Batch54/60]-[Loss:0.093841]-[Loss_scale:41.017105]\n",
      "[Epoch1/1]-[Batch55/60]-[Loss:0.093575]-[Loss_scale:40.883778]\n",
      "[Epoch1/1]-[Batch56/60]-[Loss:0.093323]-[Loss_scale:40.756863]\n",
      "[Epoch1/1]-[Batch57/60]-[Loss:0.093083]-[Loss_scale:40.635456]\n",
      "[Epoch1/1]-[Batch58/60]-[Loss:0.092809]-[Loss_scale:40.498875]\n",
      "[Epoch1/1]-[Batch59/60]-[Loss:0.092559]-[Loss_scale:40.372219]\n",
      "[Epoch1/1]-[Batch0/60]-[Loss:0.351981]-[Loss_scale:156.675797]\n",
      "[Epoch1/1]-[Batch1/60]-[Loss:0.306419]-[Loss_scale:136.126694]\n",
      "[Epoch1/1]-[Batch2/60]-[Loss:0.270141]-[Loss_scale:119.816330]\n",
      "[Epoch1/1]-[Batch3/60]-[Loss:0.239985]-[Loss_scale:106.299896]\n",
      "[Epoch1/1]-[Batch4/60]-[Loss:0.215910]-[Loss_scale:95.535538]\n",
      "[Epoch1/1]-[Batch5/60]-[Loss:0.197284]-[Loss_scale:87.223068]\n",
      "[Epoch1/1]-[Batch6/60]-[Loss:0.182715]-[Loss_scale:80.729172]\n",
      "[Epoch1/1]-[Batch7/60]-[Loss:0.171255]-[Loss_scale:75.623726]\n",
      "[Epoch1/1]-[Batch8/60]-[Loss:0.162018]-[Loss_scale:71.510323]\n",
      "[Epoch1/1]-[Batch9/60]-[Loss:0.154617]-[Loss_scale:68.213249]\n",
      "[Epoch1/1]-[Batch10/60]-[Loss:0.148120]-[Loss_scale:65.317650]\n",
      "[Epoch1/1]-[Batch11/60]-[Loss:0.142866]-[Loss_scale:62.974102]\n",
      "[Epoch1/1]-[Batch12/60]-[Loss:0.138235]-[Loss_scale:60.907742]\n",
      "[Epoch1/1]-[Batch13/60]-[Loss:0.134257]-[Loss_scale:59.131046]\n",
      "[Epoch1/1]-[Batch14/60]-[Loss:0.130777]-[Loss_scale:57.575294]\n",
      "[Epoch1/1]-[Batch15/60]-[Loss:0.127733]-[Loss_scale:56.213295]\n",
      "[Epoch1/1]-[Batch16/60]-[Loss:0.125135]-[Loss_scale:55.048164]\n",
      "[Epoch1/1]-[Batch17/60]-[Loss:0.122825]-[Loss_scale:54.011173]\n",
      "[Epoch1/1]-[Batch18/60]-[Loss:0.120736]-[Loss_scale:53.071732]\n",
      "[Epoch1/1]-[Batch19/60]-[Loss:0.118754]-[Loss_scale:52.179943]\n",
      "[Epoch1/1]-[Batch20/60]-[Loss:0.116974]-[Loss_scale:51.377964]\n",
      "[Epoch1/1]-[Batch21/60]-[Loss:0.115376]-[Loss_scale:50.656097]\n",
      "[Epoch1/1]-[Batch22/60]-[Loss:0.113984]-[Loss_scale:50.025879]\n",
      "[Epoch1/1]-[Batch23/60]-[Loss:0.112647]-[Loss_scale:49.419262]\n",
      "[Epoch1/1]-[Batch24/60]-[Loss:0.111408]-[Loss_scale:48.856525]\n",
      "[Epoch1/1]-[Batch25/60]-[Loss:0.110205]-[Loss_scale:48.309280]\n",
      "[Epoch1/1]-[Batch26/60]-[Loss:0.109130]-[Loss_scale:47.818722]\n",
      "[Epoch1/1]-[Batch27/60]-[Loss:0.108248]-[Loss_scale:47.413174]\n",
      "[Epoch1/1]-[Batch28/60]-[Loss:0.107344]-[Loss_scale:46.998207]\n",
      "[Epoch1/1]-[Batch29/60]-[Loss:0.106399]-[Loss_scale:46.565449]\n",
      "[Epoch1/1]-[Batch30/60]-[Loss:0.105634]-[Loss_scale:46.211464]\n",
      "[Epoch1/1]-[Batch31/60]-[Loss:0.104838]-[Loss_scale:45.844009]\n",
      "[Epoch1/1]-[Batch32/60]-[Loss:0.104060]-[Loss_scale:45.485336]\n",
      "[Epoch1/1]-[Batch33/60]-[Loss:0.103325]-[Loss_scale:45.144844]\n",
      "[Epoch1/1]-[Batch34/60]-[Loss:0.102674]-[Loss_scale:44.841923]\n",
      "[Epoch1/1]-[Batch35/60]-[Loss:0.102000]-[Loss_scale:44.529041]\n",
      "[Epoch1/1]-[Batch36/60]-[Loss:0.101483]-[Loss_scale:44.284885]\n",
      "[Epoch1/1]-[Batch37/60]-[Loss:0.100918]-[Loss_scale:44.019745]\n",
      "[Epoch1/1]-[Batch38/60]-[Loss:0.100380]-[Loss_scale:43.766129]\n",
      "[Epoch1/1]-[Batch39/60]-[Loss:0.099850]-[Loss_scale:43.517269]\n",
      "[Epoch1/1]-[Batch40/60]-[Loss:0.099323]-[Loss_scale:43.269627]\n",
      "[Epoch1/1]-[Batch41/60]-[Loss:0.098842]-[Loss_scale:43.042339]\n",
      "[Epoch1/1]-[Batch42/60]-[Loss:0.098444]-[Loss_scale:42.851486]\n",
      "[Epoch1/1]-[Batch43/60]-[Loss:0.098017]-[Loss_scale:42.647858]\n",
      "[Epoch1/1]-[Batch44/60]-[Loss:0.097628]-[Loss_scale:42.460030]\n",
      "[Epoch1/1]-[Batch45/60]-[Loss:0.097329]-[Loss_scale:42.312374]\n",
      "[Epoch1/1]-[Batch46/60]-[Loss:0.096944]-[Loss_scale:42.126904]\n",
      "[Epoch1/1]-[Batch47/60]-[Loss:0.096592]-[Loss_scale:41.955421]\n",
      "[Epoch1/1]-[Batch48/60]-[Loss:0.096281]-[Loss_scale:41.802368]\n",
      "[Epoch1/1]-[Batch49/60]-[Loss:0.095935]-[Loss_scale:41.633636]\n",
      "[Epoch1/1]-[Batch50/60]-[Loss:0.095589]-[Loss_scale:41.465145]\n",
      "[Epoch1/1]-[Batch51/60]-[Loss:0.095212]-[Loss_scale:41.282932]\n",
      "[Epoch1/1]-[Batch52/60]-[Loss:0.094867]-[Loss_scale:41.115295]\n",
      "[Epoch1/1]-[Batch53/60]-[Loss:0.094543]-[Loss_scale:40.955544]\n",
      "[Epoch1/1]-[Batch54/60]-[Loss:0.094214]-[Loss_scale:40.795418]\n",
      "[Epoch1/1]-[Batch55/60]-[Loss:0.093887]-[Loss_scale:40.635334]\n",
      "[Epoch1/1]-[Batch56/60]-[Loss:0.093581]-[Loss_scale:40.484138]\n",
      "[Epoch1/1]-[Batch57/60]-[Loss:0.093336]-[Loss_scale:40.359554]\n",
      "[Epoch1/1]-[Batch58/60]-[Loss:0.093053]-[Loss_scale:40.218575]\n",
      "[Epoch1/1]-[Batch59/60]-[Loss:0.092769]-[Loss_scale:40.077724]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "N_samples = len(train_data)\n",
    "print(N_samples)\n",
    "idx = np.arange(N_samples)\n",
    "np.random.shuffle(idx)\n",
    "idx\n",
    "\n",
    "\n",
    "stride =45\n",
    "window_size = 120\n",
    "# Create train sets\n",
    "shifts = np.floor(N_samples / stride)\n",
    "train_sets = int(shifts)\n",
    "print('train_sets: ', train_sets)\n",
    "print('window_size: ', window_size)\n",
    "train_ind_ls = []\n",
    "\n",
    "for i in range(train_sets):\n",
    "    train_ind_ls.append(idx[np.arange(i * stride, i * stride + window_size) % N_samples])\n",
    "\n",
    "\n",
    "\n",
    "recon_error_ls = []\n",
    "\n",
    "## Preset model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "backbone = models.resnet18(pretrained=True).to(device)\n",
    "backbone.eval()\n",
    "outputs = []\n",
    "def hook(module, input, output):\n",
    "    outputs.append(output)\n",
    "backbone.layer1[-1].register_forward_hook(hook)\n",
    "backbone.layer2[-1].register_forward_hook(hook)\n",
    "backbone.layer3[-1].register_forward_hook(hook)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def embedding_concat(x, y):\n",
    "    B, C1, H1, W1 = x.size()\n",
    "    _, C2, H2, W2 = y.size()\n",
    "    s = int(H1 / H2)\n",
    "    x = F.unfold(x, kernel_size=s, dilation=1, stride=s)\n",
    "    x = x.view(B, C1, -1, H2, W2)\n",
    "    z = torch.zeros(B, C1 + C2, x.size(2), H2, W2).to(device)\n",
    "    for i in range(x.size(2)):\n",
    "        z[:, :, i, :, :] = torch.cat((x[:, :, i, :, :], y), 1)\n",
    "    z = z.view(B, -1, H2 * W2)\n",
    "    z = F.fold(z, kernel_size=s, output_size=(H1, W1), stride=s)\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "for j in range(train_sets):\n",
    "    \n",
    "    model_result_m=args.model_result_dir+'_j_'+str(j)\n",
    "    \n",
    "    EXPERIMENT_PATH = os.path.join(args.results_dir,args.data_set ,f'contamination_{int(args.contamination_rate*100)}',f'{args.exp_name}-{args.data_category}')\n",
    "    SAVE_DIR= os.path.join(EXPERIMENT_PATH, model_result_m, 'checkpoint.pth')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(EXPERIMENT_PATH, model_result_m)):\n",
    "        os.makedirs(os.path.join(EXPERIMENT_PATH, model_result_m))\n",
    "\n",
    "    paths_j = [train_data[idx] for idx in train_ind_ls[j]]\n",
    "\n",
    "    dataset_j=ImageDataset_mvtec(args,DATA_PATH,mode='train',train_paths = paths_j, test_paths = None)\n",
    "    train_j_dataloader = DataLoader(dataset_j, batch_size=2,shuffle=True,num_workers=8,drop_last=False)\n",
    "    \n",
    "    #train_dataloader, valid_loader ,test_dataloader = get_dataloader(args)\n",
    "\n",
    "    ## Train model\n",
    "    \n",
    "    # #model....\n",
    "    start_epoch = 0\n",
    "    transformer = Create_nets(args)\n",
    "    transformer = transformer.to(device)\n",
    "    transformer.cuda()\n",
    "    optimizer = torch.optim.Adam( transformer.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n",
    "    best_loss = 1e10\n",
    "    \n",
    "    \n",
    "    #### TRAIN MODEL\n",
    "    for epoch in range(start_epoch, args.epoch_num):\n",
    "        avg_loss = 0\n",
    "        avg_loss_scale = 0\n",
    "        total = 0\n",
    "        transformer.train()\n",
    "        for i,(filename, batch) in enumerate(train_j_dataloader):\n",
    "            \n",
    "            inputs = batch.to(device)\n",
    "            outputs = []\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _ = backbone(inputs)\n",
    "\n",
    "                outputs = embedding_concat(embedding_concat(outputs[0],outputs[1]),outputs[2])\n",
    "        \n",
    "            recon, std = transformer(outputs)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            loss = criterion(recon, outputs)\n",
    "            loss_scale = criterion(std, torch.norm(recon - outputs, p = 2, dim = 1, keepdim = True).detach())\n",
    "            (loss+loss_scale).backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            avg_loss += loss * inputs.size(0)\n",
    "            avg_loss_scale += loss_scale * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "            print((\"\\r[Epoch%d/%d]-[Batch%d/%d]-[Loss:%f]-[Loss_scale:%f]\" %\n",
    "                                                            (epoch+1, args.epoch_num,\n",
    "                                                            i, len(train_j_dataloader),\n",
    "                                                            avg_loss / total,\n",
    "                                                            avg_loss_scale / total)))\n",
    "\n",
    "\n",
    "        if best_loss > avg_loss and best_loss > loss:\n",
    "            best_loss = avg_loss\n",
    "            state_dict = {\n",
    "                        'start_epoch':epoch,\n",
    "                        #'optimizer':optimizer.state_dict(),\n",
    "                        'transformer':transformer.state_dict(),\n",
    "                        'args':args,\n",
    "                        'best_loss':best_loss\n",
    "                }\n",
    "            torch.save(state_dict, SAVE_DIR)\n",
    "            \n",
    "        # EVALUATE ON TRAINING SET\n",
    "        print(\"start evaluation on test set!\")\n",
    "        transformer.eval()\n",
    "        score_map = []\n",
    "        gt_list = []\n",
    "        gt_mask_list = []\n",
    "        for i,(name ,batch, ground_truth, gt) in enumerate(test_dataloader):\n",
    "            with torch.no_grad():\n",
    "                inputs = batch.to(device)\n",
    "                ground_truth = ground_truth.to(device)\n",
    "                outputs = []\n",
    "                _ = backbone(inputs)\n",
    "                outputs = embedding_concat(embedding_concat(outputs[0],outputs[1]),outputs[2])\n",
    "                recon, std = transformer(outputs)\n",
    "                \n",
    "                \n",
    "                \n",
    "                batch_size, channels, width, height = recon.size()\n",
    "                dist = torch.norm(recon - outputs, p = 2, dim = 1, keepdim = True).div(std.abs())\n",
    "                dist = dist.view(batch_size, 1, width, height)\n",
    "                patch_normed_score = []\n",
    "                for j in range(4):\n",
    "                    patch_size = pow(4, j)\n",
    "                    patch_score = F.conv2d(input=dist, \n",
    "                        weight=(torch.ones(1,1,patch_size,patch_size) / (patch_size*patch_size)).to(device), \n",
    "                        bias=None, stride=patch_size, padding=0, dilation=1)\n",
    "                    patch_score = F.avg_pool2d(dist,patch_size,patch_size)\n",
    "                    patch_score = F.interpolate(patch_score, (width,height), mode='bilinear', align_corners=False)\n",
    "                    patch_normed_score.append(patch_score)\n",
    "                score = torch.zeros(batch_size,1,64,64).to(device)\n",
    "                for j in range(4):\n",
    "                    score = embedding_concat(score, patch_normed_score[j])\n",
    "                \n",
    "                score = F.conv2d(input=score, \n",
    "                        weight=torch.tensor([[[[0.0]],[[0.25]],[[0.25]],[[0.25]],[[0.25]]]]).to(device), \n",
    "                        bias=None, stride=1, padding=0, dilation=1)\n",
    "                score = F.interpolate(score, (ground_truth.size(2),ground_truth.size(3)), mode='bilinear', align_corners=False)\n",
    "                heatmap = score.repeat(1,3,1,1)\n",
    "                score_map.append(score.cpu())\n",
    "                gt_mask_list.append(ground_truth.cpu())\n",
    "                gt_list.append(gt)\n",
    "        \n",
    "#         score_map = torch.cat(score_map,dim=0)\n",
    "        \n",
    "#         gt_mask_list = torch.cat(gt_mask_list,dim=0)\n",
    "#         gt_list = torch.cat(gt_list,dim=0)\n",
    "\n",
    "#         # Normalization\n",
    "#         max_score = score_map.max()\n",
    "#         min_score = score_map.min()\n",
    "#         scores = (score_map - min_score) / (max_score - min_score)\n",
    "        \n",
    "#         # calculate image-level ROC AUC score\n",
    "#         img_scores = scores.view(scores.size(0),-1).max(dim=1)[0]\n",
    "#         gt_list = gt_list.numpy()\n",
    "#         fpr, tpr, _ = roc_curve(gt_list, img_scores)\n",
    "#         img_roc_auc = roc_auc_score(gt_list, img_scores)\n",
    "#         print('image ROCAUC: %.3f' % (img_roc_auc))\n",
    "\n",
    "#         # calculate per-pixel level ROCAUC\n",
    "#         gt_mask = gt_mask_list.numpy().astype('int')\n",
    "#         scores = scores.numpy().astype('float32')\n",
    "#         fpr, tpr, thresholds = roc_curve(gt_mask.flatten(), scores.flatten()) \n",
    "#         per_pixel_rocauc = roc_auc_score(gt_mask.flatten(), scores.flatten()) \n",
    "#         print('pixel ROCAUC: %.3f' % (per_pixel_rocauc))\n",
    "\n",
    "#         with open(os.path.join(EXPERIMENT_PATH,'args.log') ,\"a\") as train_log:\n",
    "#             train_log.write(\"\\r[Epoch%d]-[Loss:%f]-[Loss_scale:%f]-[image_AUC:%f]-[pixel_AUC:%f]\" %\n",
    "#                                                         (epoch+1, avg_loss / total, avg_loss_scale / total, img_roc_auc, per_pixel_rocauc))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     #   Recon_score for train_ind_ls[i] \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     recon_error_ls.append(test_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Look at recon errors of each run\n",
    "# scores = np.vstack(recon_error_ls)\n",
    "\n",
    "# #scale reconstruction errors for each run to make them comparable\n",
    "# for i in range(train_sets):\n",
    "#     scores[i, :] = (scores[i, :] - scores[i, train_ind_ls[i]].mean()) / scores[i, train_ind_ls[i]].std()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save scores\n",
    "np.savetxt(os.path.join(config['exp_path'], config['mode'] + '_scores.csv'), scores, delimiter=',')\n",
    "\n",
    "# Create boolean matrix for train and test points\n",
    "scores_bool = np.zeros_like(scores, dtype=bool)\n",
    "\n",
    "for i in range(train_sets):\n",
    "    scores_bool[i, train_ind_ls[i]] = True\n",
    "\n",
    "np.savetxt(os.path.join(config['exp_path'], config['mode'] + '_scores_bool.csv'), scores_bool, delimiter=',')\n",
    "\n",
    "#Splitt scores in train and test Scores\n",
    "scores_test = scores.copy()\n",
    "scores_train = scores.copy()\n",
    "\n",
    "scores_test[scores_bool] = np.nan\n",
    "scores_train[~scores_bool] = np.nan\n",
    "\n",
    "# Check how much point is varying in train and test\n",
    "scores_test_mean = np.nanmean(scores_test,axis = 0)\n",
    "scores_test_std = np.nanstd(scores_test,axis = 0)\n",
    "scores_train_mean = np.nanmean(scores_train,axis = 0)\n",
    "scores_train_std = np.nanstd(scores_train,axis = 0)\n",
    "\n",
    "indicator = np.abs(scores_test_mean - scores_train_mean)\n",
    "\n",
    "# Calculate cutoff for refinement\n",
    "cutoff = np.quantile(indicator, 1-assumed_contamination_rate)\n",
    "\n",
    "data_annot = copy.deepcopy(data)\n",
    "data_annot.labels = np.zeros_like(data.labels)\n",
    "data_annot.labels[indicator > cutoff] = 1\n",
    "\n",
    "# Train the model on the refined data\n",
    "test_score, holdout_score = SemiSup(data_annot, holdout_data, ADtrainer, ADmodel, net, loss, logger, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def USDR(data, holdout_data, ADtrainer, ADmodel, net, loss, logger, config):\n",
    "    \"\"\"\n",
    "    Unsupervised Data Refinement (USDR) framework.\n",
    "\n",
    "    Args:\n",
    "        data: The training data.\n",
    "        holdout_data: The holdout data for evaluation.\n",
    "        ADtrainer: The anomaly detection trainer class.\n",
    "        ADmodel: The anomaly detection model class.\n",
    "        net: The neural network architecture.\n",
    "        loss: The loss function.\n",
    "        logger: The logger for logging.\n",
    "        config: The configuration parameters.\n",
    "\n",
    "    Returns:\n",
    "        indicator: The indicator values.\n",
    "        test_score: The test score.\n",
    "        holdout_score: The holdout score.\n",
    "    \"\"\"\n",
    "    # Extracting configuration parameters\n",
    "    window_size = config['window_size']\n",
    "    stride = config['stride']\n",
    "    assumed_contamination_rate = config['assumed_contamination_rate']\n",
    "    \n",
    "    # Shuffle the data indices\n",
    "    N = data.__len__()\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    # Create train sets\n",
    "    shifts = np.floor(N / stride)\n",
    "    train_sets = int(shifts)\n",
    "    print('train_sets: ', train_sets)\n",
    "    print('window_size: ', window_size)\n",
    "    train_ind_ls = []\n",
    "\n",
    "    test_data = copy.deepcopy(data)\n",
    "    test_loader = DataLoader(test_data, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    for i in range(train_sets):\n",
    "        train_ind_ls.append(idx[np.arange(i * stride, i * stride + window_size) % N])\n",
    "\n",
    "    # Train the model on each train set\n",
    "\n",
    "    recon_error_ls = []\n",
    "    \n",
    "    for i in range(train_sets):\n",
    "        \n",
    "        train_m_dataloader = DataLoader(ImageDataset_mvtec(args,DATA_PATH,mode='train',train_paths = normal_images,test_paths = None), batch_size=2,shuffle=True,num_workers=8,drop_last=False)\n",
    "\n",
    "        recon_error_ls.append(test_score)\n",
    "\n",
    "\n",
    "    # Look at recon errors of each run\n",
    "    scores = np.vstack(recon_error_ls)\n",
    "\n",
    "    #scale reconstruction errors for each run to make them comparable\n",
    "    for i in range(train_sets):\n",
    "        scores[i, :] = (scores[i, :] - scores[i, train_ind_ls[i]].mean()) / scores[i, train_ind_ls[i]].std()\n",
    "\n",
    "    #save scores\n",
    "    np.savetxt(os.path.join(config['exp_path'], config['mode'] + '_scores.csv'), scores, delimiter=',')\n",
    "\n",
    "    # Create boolean matrix for train and test points\n",
    "    scores_bool = np.zeros_like(scores, dtype=bool)\n",
    "\n",
    "    for i in range(train_sets):\n",
    "        scores_bool[i, train_ind_ls[i]] = True\n",
    "\n",
    "    np.savetxt(os.path.join(config['exp_path'], config['mode'] + '_scores_bool.csv'), scores_bool, delimiter=',')\n",
    "\n",
    "    #Splitt scores in train and test Scores\n",
    "    scores_test = scores.copy()\n",
    "    scores_train = scores.copy()\n",
    "\n",
    "    scores_test[scores_bool] = np.nan\n",
    "    scores_train[~scores_bool] = np.nan\n",
    "\n",
    "    # Check how much point is varying in train and test\n",
    "    scores_test_mean = np.nanmean(scores_test,axis = 0)\n",
    "    scores_test_std = np.nanstd(scores_test,axis = 0)\n",
    "    scores_train_mean = np.nanmean(scores_train,axis = 0)\n",
    "    scores_train_std = np.nanstd(scores_train,axis = 0)\n",
    "\n",
    "    indicator = np.abs(scores_test_mean - scores_train_mean)\n",
    "    \n",
    "    # Calculate cutoff for refinement\n",
    "    cutoff = np.quantile(indicator, 1-assumed_contamination_rate)\n",
    "\n",
    "    data_annot = copy.deepcopy(data)\n",
    "    data_annot.labels = np.zeros_like(data.labels)\n",
    "    data_annot.labels[indicator > cutoff] = 1\n",
    "\n",
    "    # Train the model on the refined data\n",
    "    test_score, holdout_score = SemiSup(data_annot, holdout_data, ADtrainer, ADmodel, net, loss, logger, config)\n",
    "\n",
    "    return indicator, test_score, holdout_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
